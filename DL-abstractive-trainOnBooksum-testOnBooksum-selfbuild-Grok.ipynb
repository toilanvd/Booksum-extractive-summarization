{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ce42a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Viet-Dung\n",
      "[nltk_data]     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Viet-Dung\n",
      "[nltk_data]     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "INFO:__main__:Using device: cuda\n",
      "Building Extractive Vocabulary: 100%|███████| 2308987/2308987 [02:26<00:00, 15807.58it/s]\n",
      "Training Extractive Batches: 100%|███████████████████| 1200/1200 [01:23<00:00, 14.45it/s]\n",
      "INFO:__main__:Epoch 1, Train Loss: 0.3213, Val Loss: 0.3393\n",
      "Training Extractive Batches: 100%|███████████████████| 1200/1200 [01:24<00:00, 14.13it/s]\n",
      "INFO:__main__:Epoch 2, Train Loss: 0.2974, Val Loss: 0.3285\n",
      "Training Extractive Batches: 100%|███████████████████| 1200/1200 [01:24<00:00, 14.28it/s]\n",
      "INFO:__main__:Epoch 3, Train Loss: 0.2876, Val Loss: 0.3318\n",
      "Training Extractive Batches: 100%|███████████████████| 1200/1200 [01:26<00:00, 13.89it/s]\n",
      "INFO:__main__:Epoch 4, Train Loss: 0.2826, Val Loss: 0.3357\n",
      "INFO:__main__:Early stopping triggered\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "field larger than field limit (131072)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mError\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:805\u001b[39m, in \u001b[36mPythonParser._next_iter_line\u001b[39m\u001b[34m(self, row_num)\u001b[39m\n\u001b[32m    804\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m805\u001b[39m line = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    806\u001b[39m \u001b[38;5;66;03m# for mypy\u001b[39;00m\n",
      "\u001b[31mError\u001b[39m: field larger than field limit (131072)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mParserError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 475\u001b[39m\n\u001b[32m    472\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[38;5;66;03m# Step 2: Generate extractive summaries\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m train_data = \u001b[43mload_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mDataset/train.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m,\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    476\u001b[39m dev_data = load_csv(\u001b[33m'\u001b[39m\u001b[33mDataset/dev.csv\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    477\u001b[39m test_data = load_csv(\u001b[33m'\u001b[39m\u001b[33mDataset/test.csv\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mload_csv\u001b[39m\u001b[34m(file_path, delimiter_symbol)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_csv\u001b[39m(file_path, delimiter_symbol):\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdelimiter_symbol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpython\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43miso-8859-1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m:\n\u001b[32m     33\u001b[39m         logger.warning(\u001b[33m\"\u001b[39m\u001b[33mISO-8859-1 encoding failed, trying latin1\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:252\u001b[39m, in \u001b[36mPythonParser.read\u001b[39m\u001b[34m(self, rows)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread\u001b[39m(\n\u001b[32m    247\u001b[39m     \u001b[38;5;28mself\u001b[39m, rows: \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    248\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[\n\u001b[32m    249\u001b[39m     Index | \u001b[38;5;28;01mNone\u001b[39;00m, Sequence[Hashable] | MultiIndex, Mapping[Hashable, ArrayLike]\n\u001b[32m    250\u001b[39m ]:\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m         content = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    253\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    254\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._first_chunk:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:1140\u001b[39m, in \u001b[36mPythonParser._get_lines\u001b[39m\u001b[34m(self, rows)\u001b[39m\n\u001b[32m   1137\u001b[39m rows = \u001b[32m0\u001b[39m\n\u001b[32m   1139\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1140\u001b[39m     next_row = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_iter_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow_num\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1141\u001b[39m     rows += \u001b[32m1\u001b[39m\n\u001b[32m   1143\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m next_row \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:834\u001b[39m, in \u001b[36mPythonParser._next_iter_line\u001b[39m\u001b[34m(self, row_num)\u001b[39m\n\u001b[32m    825\u001b[39m         reason = (\n\u001b[32m    826\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mError could possibly be due to \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    827\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mparsing errors in the skipped footer rows \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    830\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mall rows).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    831\u001b[39m         )\n\u001b[32m    832\u001b[39m         msg += \u001b[33m\"\u001b[39m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m + reason\n\u001b[32m--> \u001b[39m\u001b[32m834\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_alert_malformed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_num\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:781\u001b[39m, in \u001b[36mPythonParser._alert_malformed\u001b[39m\u001b[34m(self, msg, row_num)\u001b[39m\n\u001b[32m    764\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    765\u001b[39m \u001b[33;03mAlert a user about a malformed row, depending on value of\u001b[39;00m\n\u001b[32m    766\u001b[39m \u001b[33;03m`self.on_bad_lines` enum.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    778\u001b[39m \u001b[33;03m    even though we 0-index internally.\u001b[39;00m\n\u001b[32m    779\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    780\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.on_bad_lines == \u001b[38;5;28mself\u001b[39m.BadLineHandleMethod.ERROR:\n\u001b[32m--> \u001b[39m\u001b[32m781\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ParserError(msg)\n\u001b[32m    782\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.on_bad_lines == \u001b[38;5;28mself\u001b[39m.BadLineHandleMethod.WARN:\n\u001b[32m    783\u001b[39m     warnings.warn(\n\u001b[32m    784\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSkipping line \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    785\u001b[39m         ParserWarning,\n\u001b[32m    786\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    787\u001b[39m     )\n",
      "\u001b[31mParserError\u001b[39m: field larger than field limit (131072)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from rouge_score import rouge_scorer\n",
    "import random\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f'Using device: {device}')\n",
    "\n",
    "# Function to load CSV with proper encoding\n",
    "def load_csv(file_path, delimiter_symbol):\n",
    "    try:\n",
    "        return pd.read_csv(file_path, delimiter=delimiter_symbol, engine='python', encoding='iso-8859-1')\n",
    "    except UnicodeDecodeError:\n",
    "        logger.warning(\"ISO-8859-1 encoding failed, trying latin1\")\n",
    "        return pd.read_csv(file_path, delimiter=delimiter_symbol, engine='python', encoding='latin1')\n",
    "\n",
    "# Build vocabulary for extractive model\n",
    "def build_vocab(csv_file, min_freq=2):\n",
    "    data = load_csv(csv_file, \";;;;;;\")\n",
    "    texts = data['content'].astype(str).tolist()\n",
    "    word_counts = Counter()\n",
    "    for text in tqdm(texts, desc=\"Building Extractive Vocabulary\"):\n",
    "        words = word_tokenize(text.lower())\n",
    "        word_counts.update(words)\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    idx = len(vocab)\n",
    "    for word, count in word_counts.items():\n",
    "        if count >= min_freq:\n",
    "            vocab[word] = idx\n",
    "            idx += 1\n",
    "    return vocab\n",
    "\n",
    "# Custom Dataset for extractive model\n",
    "class SummaryDataset(Dataset):\n",
    "    def __init__(self, csv_file, vocab, max_length=40, max_sentences=7000):\n",
    "        self.data = load_csv(csv_file, \";;;;;;\")\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.max_length = max_length\n",
    "        self.max_sentences = max_sentences\n",
    "        self.chapters = []\n",
    "        self.chapter_data = []\n",
    "        \n",
    "        # Group data by chapter\n",
    "        grouped = self.data.groupby('chapter')\n",
    "        for chapter_name, group in grouped:\n",
    "            # Sort by sentence ID to ensure correct order\n",
    "            group = group.sort_values('sentence', ascending=True)\n",
    "            texts = []\n",
    "            labels = []\n",
    "            sentence_ids = []\n",
    "            tokenized_indices = []\n",
    "            \n",
    "            for idx, row in group.iterrows():\n",
    "                text = str(row['content'])\n",
    "                label = row['in_summary']\n",
    "                sentence_id = row['sentence']\n",
    "                try:\n",
    "                    label_int = int(label)\n",
    "                    if label_int not in [0, 1]:\n",
    "                        logger.warning(f\"Skipping row {idx}: Invalid label value {label}\")\n",
    "                        continue\n",
    "                    sentence_id_int = int(sentence_id)\n",
    "                except (ValueError, TypeError):\n",
    "                    logger.warning(f\"Skipping row {idx}: Invalid label {label} or sentence ID {sentence_id}\")\n",
    "                    continue\n",
    "                \n",
    "                # Pre-tokenize\n",
    "                words = word_tokenize(text.lower())\n",
    "                indices = [self.vocab.get(word, self.vocab['<UNK>']) for word in words]\n",
    "                if len(indices) > max_length:\n",
    "                    indices = indices[:max_length]\n",
    "                else:\n",
    "                    indices += [self.vocab['<PAD>']] * (max_length - len(indices))\n",
    "                \n",
    "                # Validate indices\n",
    "                indices = [max(0, min(idx, self.vocab_size - 1)) for idx in indices]\n",
    "                if any(idx < 0 or idx >= self.vocab_size for idx in indices):\n",
    "                    logger.warning(f\"Invalid indices in text at index {idx}: {indices}\")\n",
    "                    indices = [self.vocab['<PAD>']] * max_length\n",
    "                \n",
    "                texts.append(text)\n",
    "                labels.append(label_int)\n",
    "                sentence_ids.append(sentence_id_int)\n",
    "                tokenized_indices.append(indices)\n",
    "            \n",
    "            if texts:\n",
    "                # Truncate at initialization to avoid oversized chapters\n",
    "                if len(tokenized_indices) > max_sentences:\n",
    "                    logger.warning(f\"Chapter {chapter_name} has {len(tokenized_indices)} sentences, truncating to {max_sentences}\")\n",
    "                    tokenized_indices = tokenized_indices[:max_sentences]\n",
    "                    labels = labels[:max_sentences]\n",
    "                    texts = texts[:max_sentences]\n",
    "                    sentence_ids = sentence_ids[:max_sentences]\n",
    "                self.chapters.append(chapter_name)\n",
    "                self.chapter_data.append({\n",
    "                    'texts': texts,\n",
    "                    'labels': labels,\n",
    "                    'sentence_ids': sentence_ids,\n",
    "                    'tokenized_indices': tokenized_indices\n",
    "                })\n",
    "        \n",
    "        if not self.chapters:\n",
    "            raise ValueError(\"No valid chapters found after cleaning\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chapters)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chapter = self.chapter_data[idx]\n",
    "        tokenized_indices = chapter['tokenized_indices']\n",
    "        labels = chapter['labels']\n",
    "        num_sentences = len(tokenized_indices)  # Number of sentences after init truncation\n",
    "        \n",
    "        # Convert to tensors without padding to max_sentences\n",
    "        input_ids = torch.tensor(tokenized_indices, dtype=torch.long)  # Shape: [num_sentences, max_length]\n",
    "        labels = torch.tensor(labels, dtype=torch.long)               # Shape: [num_sentences]\n",
    "        \n",
    "        # Verify tensor sizes\n",
    "        assert num_sentences <= self.max_sentences, \\\n",
    "            f\"num_sentences {num_sentences} > max_sentences {self.max_sentences} for chapter {self.chapters[idx]}\"\n",
    "        assert input_ids.size() == (num_sentences, self.max_length), \\\n",
    "            f\"input_ids size {input_ids.size()} != [{num_sentences}, {self.max_length}] for chapter {self.chapters[idx]}\"\n",
    "        assert labels.size() == (num_sentences,), \\\n",
    "            f\"labels size {labels.size()} != [{num_sentences}] for chapter {self.chapters[idx]}\"\n",
    "        \n",
    "        logger.debug(f\"Chapter {self.chapters[idx]}: {num_sentences} sentences, input_ids shape {input_ids.size()}\")\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,  # Shape: [num_sentences, max_length]\n",
    "            'labels': labels,        # Shape: [num_sentences]\n",
    "            'num_sentences': num_sentences  # Actual number of sentences\n",
    "        }\n",
    "\n",
    "# Extractive Model Definition\n",
    "class SummaryLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=64, num_layers=1):\n",
    "        super(SummaryLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x, num_sentences):\n",
    "        # x shape: [batch_size, max_sentences, max_length]\n",
    "        batch_size = x.size(0)\n",
    "        outputs = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # Process sentences for one chapter\n",
    "            chapter_input = x[i, :num_sentences[i]]  # Shape: [num_sentences, max_length]\n",
    "            embedded = self.embedding(chapter_input)  # Shape: [num_sentences, max_length, embedding_dim]\n",
    "            # Average embeddings over words to get sentence embeddings\n",
    "            sentence_embeds = embedded.mean(dim=1)    # Shape: [num_sentences, embedding_dim]\n",
    "            lstm_out, (hidden, _) = self.lstm(sentence_embeds.unsqueeze(0))  # Shape: [1, num_sentences, hidden_dim*2]\n",
    "            lstm_out = lstm_out.squeeze(0)  # Shape: [num_sentences, hidden_dim*2]\n",
    "            lstm_out = self.dropout(lstm_out)\n",
    "            output = self.fc(lstm_out)  # Shape: [num_sentences, 2]\n",
    "            # Pad output to max_sentences\n",
    "            if num_sentences[i] < x.size(1):\n",
    "                pad_size = x.size(1) - num_sentences[i]\n",
    "                output = torch.cat([\n",
    "                    output,\n",
    "                    torch.zeros(pad_size, 2, device=output.device)\n",
    "                ], dim=0)\n",
    "            outputs.append(output)\n",
    "        \n",
    "        return torch.stack(outputs)  # Shape: [batch_size, max_sentences, 2]\n",
    "\n",
    "# Function to generate extractive summary\n",
    "def generate_summary(chapter_sentences, vocab, model, device, max_length=40, max_sentences=7000, target_ratio=0.15):\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    chapter_words = sum(len(word_tokenize(sentence)) for sentence in chapter_sentences)\n",
    "    target_word_count = int(chapter_words * target_ratio)\n",
    "    tokenized_indices = []\n",
    "    for sentence in chapter_sentences:\n",
    "        words = word_tokenize(sentence.lower())\n",
    "        indices = [vocab.get(word, vocab['<UNK>']) for word in words]\n",
    "        if len(indices) > max_length:\n",
    "            indices = indices[:max_length]\n",
    "        else:\n",
    "            indices += [vocab['<PAD>']] * (max_length - len(indices))\n",
    "        indices = [max(0, min(idx, len(vocab) - 1)) for idx in indices]\n",
    "        tokenized_indices.append(indices)\n",
    "    num_sentences = len(tokenized_indices)\n",
    "    if num_sentences > max_sentences:\n",
    "        tokenized_indices = tokenized_indices[:max_sentences]\n",
    "        chapter_sentences = chapter_sentences[:max_sentences]\n",
    "        num_sentences = max_sentences\n",
    "    elif num_sentences < max_sentences:\n",
    "        tokenized_indices += [[vocab['<PAD>']] * max_length] * (max_sentences - num_sentences)\n",
    "    input_ids = torch.tensor([tokenized_indices], dtype=torch.long).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, torch.tensor([num_sentences], device=device))\n",
    "        scores = torch.softmax(outputs[0], dim=1)[:, 1].cpu().numpy()\n",
    "    sentence_scores = [(sentence, score, len(word_tokenize(sentence)), idx) \n",
    "                      for idx, (sentence, score) in enumerate(zip(chapter_sentences, scores[:num_sentences]))]\n",
    "    sentence_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    selected = []\n",
    "    current_word_count = 0\n",
    "    for sentence, score, word_count, sentence_id in sentence_scores:\n",
    "        if current_word_count + word_count <= target_word_count or len(selected) < 1:\n",
    "            selected.append((sentence, sentence_id))\n",
    "            current_word_count += word_count\n",
    "        else:\n",
    "            break\n",
    "    selected.sort(key=lambda x: x[1])\n",
    "    selected_sentences = [sentence for sentence, _ in selected]\n",
    "    return ' '.join(selected_sentences)\n",
    "\n",
    "# Training and evaluation functions for extractive model\n",
    "def train_epoch(model, data_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(data_loader, desc=\"Training Extractive Batches\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        num_sentences = batch['num_sentences'].to(device)\n",
    "        outputs = model(input_ids, num_sentences)\n",
    "        outputs = outputs.view(-1, 2)\n",
    "        labels = labels.view(-1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def eval_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            num_sentences = batch['num_sentences'].to(device)\n",
    "            outputs = model(input_ids, num_sentences)\n",
    "            outputs = outputs.view(-1, 2)\n",
    "            labels = labels.view(-1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# Main execution\n",
    "# Step 1: Train extractive model\n",
    "vocab = build_vocab('Dataset/train_GAlabelled.csv')\n",
    "train_dataset = SummaryDataset('Dataset/train_GAlabelled.csv', vocab)\n",
    "val_dataset = SummaryDataset('Dataset/val_GAlabelled.csv', vocab)\n",
    "\n",
    "# Custom collate function to handle variable-length chapters\n",
    "def custom_collate_fn(batch):\n",
    "    max_sentences = 7000  # Same as max_sentences in SummaryDataset\n",
    "    max_length = 40     # Same as max_length in SummaryDataset\n",
    "\n",
    "    # Find the maximum number of sentences in this batch (capped at max_sentences)\n",
    "    batch_max_sentences = min(max(item['num_sentences'] for item in batch), max_sentences)\n",
    "    logger.debug(f\"Batch max sentences: {batch_max_sentences}\")\n",
    "\n",
    "    input_ids_list = []\n",
    "    labels_list = []\n",
    "    num_sentences_list = []\n",
    "\n",
    "    for item in batch:\n",
    "        input_ids = item['input_ids']  # Shape: [num_sentences, max_length]\n",
    "        labels = item['labels']        # Shape: [num_sentences]\n",
    "        num_sentences = item['num_sentences']\n",
    "\n",
    "        # Truncate if necessary\n",
    "        if num_sentences > batch_max_sentences:\n",
    "            logger.debug(f\"Truncating chapter from {num_sentences} to {batch_max_sentences} sentences\")\n",
    "            input_ids = input_ids[:batch_max_sentences]\n",
    "            labels = labels[:batch_max_sentences]\n",
    "            num_sentences = batch_max_sentences\n",
    "        # Pad if necessary\n",
    "        elif num_sentences < batch_max_sentences:\n",
    "            pad_size = batch_max_sentences - num_sentences\n",
    "            logger.debug(f\"Padding chapter from {num_sentences} to {batch_max_sentences} sentences\")\n",
    "            input_ids = torch.cat([\n",
    "                input_ids,\n",
    "                torch.zeros(pad_size, max_length, dtype=torch.long)\n",
    "            ], dim=0)\n",
    "            labels = torch.cat([\n",
    "                labels,\n",
    "                torch.full((pad_size,), -1, dtype=torch.long)\n",
    "            ], dim=0)\n",
    "\n",
    "        # Verify tensor size before stacking\n",
    "        assert input_ids.size() == (batch_max_sentences, max_length), \\\n",
    "            f\"input_ids size {input_ids.size()} != [{batch_max_sentences}, {max_length}]\"\n",
    "        assert labels.size() == (batch_max_sentences,), \\\n",
    "            f\"labels size {labels.size()} != [{batch_max_sentences}]\"\n",
    "\n",
    "        input_ids_list.append(input_ids)\n",
    "        labels_list.append(labels)\n",
    "        num_sentences_list.append(num_sentences)\n",
    "\n",
    "    # Stack into batch tensors\n",
    "    input_ids_batch = torch.stack(input_ids_list)  # Shape: [batch_size, batch_max_sentences, max_length]\n",
    "    labels_batch = torch.stack(labels_list)        # Shape: [batch_size, batch_max_sentences]\n",
    "    num_sentences_batch = torch.tensor(num_sentences_list, dtype=torch.long)\n",
    "\n",
    "    logger.debug(f\"Batch shapes: input_ids {input_ids_batch.size()}, labels {labels_batch.size()}\")\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids_batch,\n",
    "        'labels': labels_batch,\n",
    "        'num_sentences': num_sentences_batch\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, collate_fn=custom_collate_fn)\n",
    "\n",
    "model = SummaryLSTM(vocab_size=len(vocab), embedding_dim=100, hidden_dim=64, num_layers=1).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "patience = 2\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "max_epochs = 100\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss = eval_model(model, val_loader, criterion, device)\n",
    "    logger.info(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            logger.info(\"Early stopping triggered\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e062b312-3a5e-469b-8ec8-a36295fc6fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Extractive Summaries: 100%|███████████████| 1431/1431 [01:44<00:00, 13.68it/s]\n",
      "Building Abstractive Vocabulary: 100%|████████████| 25026/25026 [00:38<00:00, 655.48it/s]\n",
      "Training Abstractive:  10%|██▌                        | 114/1200 [01:19<12:35,  1.44it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 185\u001b[39m\n\u001b[32m    182\u001b[39m max_epochs_abstractive = \u001b[32m5\u001b[39m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_epochs_abstractive):\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     train_loss = \u001b[43mtrain_abstractive_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_abstractive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_abstractive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_abstractive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_abstractive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m     \u001b[38;5;66;03m# Evaluate validation loss\u001b[39;00m\n\u001b[32m    187\u001b[39m     model_abstractive.eval()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 108\u001b[39m, in \u001b[36mtrain_abstractive_epoch\u001b[39m\u001b[34m(model, loader, criterion, optimizer, device)\u001b[39m\n\u001b[32m    103\u001b[39m loss = criterion(\n\u001b[32m    104\u001b[39m     outputs.contiguous().view(-\u001b[32m1\u001b[39m, outputs.size(-\u001b[32m1\u001b[39m)),\n\u001b[32m    105\u001b[39m     decoder_target.contiguous().view(-\u001b[32m1\u001b[39m)\n\u001b[32m    106\u001b[39m )\n\u001b[32m    107\u001b[39m optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m optimizer.step()\n\u001b[32m    110\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Build vocabulary for abstractive model\n",
    "def build_vocab_abstractive(texts, min_freq=2):\n",
    "    word_counts = Counter()\n",
    "    for text in tqdm(texts, desc=\"Building Abstractive Vocabulary\"):\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        word_counts.update(tokens)\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3}\n",
    "    idx = len(vocab)\n",
    "    for word, count in word_counts.items():\n",
    "        if count >= min_freq:\n",
    "            vocab[word] = idx\n",
    "            idx += 1\n",
    "    return vocab, {v: k for k, v in vocab.items()}\n",
    "\n",
    "# Custom Dataset for abstractive model\n",
    "class AbstractiveDataset(Dataset):\n",
    "    def __init__(self, pairs, vocab, max_input_len=400, max_target_len=100):\n",
    "        self.pairs = pairs\n",
    "        self.vocab = vocab\n",
    "        self.max_input_len = max_input_len\n",
    "        self.max_target_len = max_target_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        extractive, summary = self.pairs[idx]\n",
    "        input_ids = [self.vocab.get(token, self.vocab['<UNK>']) for token in word_tokenize(extractive.lower())]\n",
    "        target_ids = [self.vocab.get(token, self.vocab['<UNK>']) for token in word_tokenize(summary.lower())]\n",
    "        input_ids = [self.vocab['<SOS>']] + input_ids + [self.vocab['<EOS>']]\n",
    "        target_ids = [self.vocab['<SOS>']] + target_ids + [self.vocab['<EOS>']]\n",
    "        input_ids = input_ids[:self.max_input_len] + [self.vocab['<PAD>']] * (self.max_input_len - len(input_ids))\n",
    "        target_ids = target_ids[:self.max_target_len] + [self.vocab['<PAD>']] * (self.max_target_len - len(target_ids))\n",
    "        return {'input_ids': torch.tensor(input_ids), 'target_ids': torch.tensor(target_ids)}\n",
    "\n",
    "# Seq2Seq Model for abstractive summarization\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (hidden, cell) = self.lstm(embedded)\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        embedded = self.embedding(x)\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        return self.fc(output), hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=128):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = Encoder(vocab_size, embedding_dim, hidden_dim)\n",
    "        self.decoder = Decoder(vocab_size, embedding_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, source, decoder_input):\n",
    "        batch_size = source.size(0)\n",
    "        seq_len = decoder_input.size(1)\n",
    "        vocab_size = self.decoder.fc.out_features\n",
    "        outputs = torch.zeros(batch_size, seq_len, vocab_size).to(source.device)\n",
    "        hidden, cell = self.encoder(source)\n",
    "        for t in range(seq_len):\n",
    "            input = decoder_input[:, t].unsqueeze(1)\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[:, t] = output.squeeze(1)\n",
    "        return outputs\n",
    "\n",
    "    def generate_summary(self, input_ids, vocab, idx2word, max_length=100):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            hidden, cell = self.encoder(input_ids.unsqueeze(0))\n",
    "            decoder_input = torch.tensor([[vocab['<SOS>']]], device=input_ids.device)\n",
    "            outputs = []\n",
    "            for _ in range(max_length):\n",
    "                output, hidden, cell = self.decoder(decoder_input, hidden, cell)\n",
    "                top_idx = output.argmax(2).item()\n",
    "                outputs.append(top_idx)\n",
    "                if top_idx == vocab['<EOS>']:\n",
    "                    break\n",
    "                decoder_input = torch.tensor([[top_idx]], device=input_ids.device)\n",
    "            return ' '.join([idx2word.get(idx, '<UNK>') for idx in outputs if idx != vocab['<EOS>']])\n",
    "\n",
    "# Training function for abstractive model\n",
    "def train_abstractive_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(loader, desc=\"Training Abstractive\"):\n",
    "        source = batch['input_ids'].to(device)\n",
    "        target_ids = batch['target_ids'].to(device)\n",
    "        decoder_input = target_ids[:, :-1]\n",
    "        decoder_target = target_ids[:, 1:]\n",
    "        outputs = model(source, decoder_input)\n",
    "        loss = criterion(\n",
    "            outputs.contiguous().view(-1, outputs.size(-1)),\n",
    "            decoder_target.contiguous().view(-1)\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Generate extractive summaries for abstractive training\n",
    "def generate_extractive_summaries(data_df, model, vocab, device):\n",
    "    summaries = []\n",
    "    for idx, row in tqdm(data_df.iterrows(), total=len(data_df), desc=\"Generating Extractive Summaries\"):\n",
    "        chapter_text = row['chapter']\n",
    "        sentences = sent_tokenize(chapter_text)\n",
    "        extractive_summary = generate_summary(sentences, vocab, model, device)\n",
    "        summaries.append((idx, extractive_summary, row['summary_text']))\n",
    "    return summaries\n",
    "\n",
    "# Function to extract summaries from labelled CSV and align with reference summaries\n",
    "def extract_summaries_with_reference(labelled_csv, reference_csv, delimiter=\";;;;;;\"):\n",
    "    labelled_data = load_csv(labelled_csv, delimiter)\n",
    "    reference_data = pd.read_csv(reference_csv)\n",
    "    \n",
    "    summaries = []\n",
    "    grouped = labelled_data.groupby('chapter')\n",
    "    labelled_chapters = list(grouped.groups.keys())\n",
    "    \n",
    "    # Ensure reference data is aligned by chapter order\n",
    "    reference_data['chapter'] = reference_data['chapter'].astype(str)\n",
    "    reference_chapters = reference_data['chapter'].tolist()\n",
    "    \n",
    "    # Verify chapter order alignment\n",
    "    if len(labelled_chapters) != len(reference_chapters):\n",
    "        logger.warning(f\"Chapter count mismatch: {len(labelled_chapters)} in labelled, {len(reference_chapters)} in reference\")\n",
    "    \n",
    "    for chapter_name, group in grouped:\n",
    "        group = group.sort_values('sentence', ascending=True)\n",
    "        extractive_summary = ' '.join(\n",
    "            str(row['content']) for _, row in group.iterrows() if int(row['in_summary']) == 1\n",
    "        )\n",
    "        # Find corresponding reference summary\n",
    "        try:\n",
    "            ref_idx = labelled_chapters.index(chapter_name)\n",
    "            ref_row = reference_data.iloc[ref_idx]\n",
    "            reference_summary = str(ref_row['summary_text'])\n",
    "        except (ValueError, IndexError):\n",
    "            logger.warning(f\"No reference summary found for chapter {chapter_name}\")\n",
    "            reference_summary = ''\n",
    "        summaries.append((chapter_name, extractive_summary, reference_summary))\n",
    "    return summaries\n",
    "\n",
    "# Step 2: Prepare abstractive data\n",
    "train_data = extract_summaries_with_reference('Dataset/train_GAlabelled.csv', 'Dataset/train.csv')\n",
    "val_data = extract_summaries_with_reference('Dataset/val_GAlabelled.csv', 'Dataset/dev.csv')\n",
    "test_data = pd.read_csv('Dataset/test.csv')\n",
    "test_abstractive_data = generate_extractive_summaries(test_data, model, vocab, device)\n",
    "\n",
    "# Step 3: Prepare abstractive dataset\n",
    "train_pairs = [(extractive, summary) for _, extractive, summary in train_data if extractive and summary]\n",
    "val_pairs = [(extractive, summary) for _, extractive, summary in val_data if extractive and summary]\n",
    "test_pairs = [(extractive, summary) for _, extractive, summary in test_abstractive_data]\n",
    "all_texts = [text for pair in (train_pairs + val_pairs + test_pairs) for text in pair if text]\n",
    "vocab_abstractive, idx2word = build_vocab_abstractive(all_texts)\n",
    "train_dataset_abstractive = AbstractiveDataset(train_pairs, vocab_abstractive)\n",
    "val_dataset_abstractive = AbstractiveDataset(val_pairs, vocab_abstractive)\n",
    "test_dataset_abstractive = AbstractiveDataset(test_pairs, vocab_abstractive)\n",
    "train_loader_abstractive = DataLoader(train_dataset_abstractive, batch_size=8, shuffle=True)\n",
    "val_loader_abstractive = DataLoader(val_dataset_abstractive, batch_size=8)\n",
    "test_loader_abstractive = DataLoader(test_dataset_abstractive, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "567d9807-655c-46f0-a580-224b73943f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Abstractive: 100%|██████████████████████████| 1200/1200 [18:57<00:00,  1.06it/s]\n",
      "INFO:__main__:Abstractive Epoch 1, Train Loss: 6.2602, Val Loss: 6.1520\n",
      "Training Abstractive: 100%|██████████████████████████| 1200/1200 [15:09<00:00,  1.32it/s]\n",
      "INFO:__main__:Abstractive Epoch 2, Train Loss: 5.4900, Val Loss: 6.0022\n",
      "Training Abstractive: 100%|██████████████████████████| 1200/1200 [16:39<00:00,  1.20it/s]\n",
      "INFO:__main__:Abstractive Epoch 3, Train Loss: 5.2086, Val Loss: 5.9417\n",
      "Training Abstractive: 100%|██████████████████████████| 1200/1200 [16:17<00:00,  1.23it/s]\n",
      "INFO:__main__:Abstractive Epoch 4, Train Loss: 5.0183, Val Loss: 5.9322\n",
      "Training Abstractive: 100%|██████████████████████████| 1200/1200 [16:17<00:00,  1.23it/s]\n",
      "INFO:__main__:Abstractive Epoch 5, Train Loss: 4.8697, Val Loss: 5.9499\n",
      "Training Abstractive: 100%|██████████████████████████| 1200/1200 [17:51<00:00,  1.12it/s]\n",
      "INFO:__main__:Abstractive Epoch 6, Train Loss: 4.7437, Val Loss: 5.9869\n",
      "Training Abstractive: 100%|██████████████████████████| 1200/1200 [17:09<00:00,  1.17it/s]\n",
      "INFO:__main__:Abstractive Epoch 7, Train Loss: 4.6322, Val Loss: 6.0136\n",
      "INFO:__main__:Early stopping triggered for abstractive model\n",
      "INFO:absl:Using default tokenizer.\n",
      "Evaluating Test Summaries: 100%|█████████████████████| 1431/1431 [03:47<00:00,  6.29it/s]\n",
      "Computing ROUGE Scores: 100%|████████████████████| 1431/1431 [00:00<00:00, 357839.92it/s]\n",
      "INFO:__main__:Average ROUGE Scores:\n",
      "INFO:__main__:rouge1: Precision=0.3691, Recall=0.1329, F1=0.1762\n",
      "INFO:__main__:rouge2: Precision=0.0483, Recall=0.0141, F1=0.0196\n",
      "INFO:__main__:rougeL: Precision=0.2755, Recall=0.0986, F1=0.1303\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Train abstractive model\n",
    "model_abstractive = Seq2Seq(len(vocab_abstractive)).to(device)\n",
    "criterion_abstractive = nn.CrossEntropyLoss(ignore_index=vocab_abstractive['<PAD>'])\n",
    "optimizer_abstractive = torch.optim.Adam(model_abstractive.parameters(), lr=0.001)\n",
    "patience = 3\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "max_epochs_abstractive = 10\n",
    "\n",
    "for epoch in range(max_epochs_abstractive):\n",
    "    train_loss = train_abstractive_epoch(model_abstractive, train_loader_abstractive, criterion_abstractive, optimizer_abstractive, device)\n",
    "    # Evaluate validation loss\n",
    "    model_abstractive.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader_abstractive:\n",
    "            source = batch['input_ids'].to(device)\n",
    "            target_ids = batch['target_ids'].to(device)\n",
    "            decoder_input = target_ids[:, :-1]\n",
    "            decoder_target = target_ids[:, 1:]\n",
    "            outputs = model_abstractive(source, decoder_input)\n",
    "            loss = criterion_abstractive(\n",
    "                outputs.contiguous().view(-1, outputs.size(-1)),\n",
    "                decoder_target.contiguous().view(-1)\n",
    "            )\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader_abstractive)\n",
    "    logger.info(f\"Abstractive Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model_abstractive.state_dict(), 'abstractive_model.pt')\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            logger.info(\"Early stopping triggered for abstractive model\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7c4c20a-b69a-46c8-9586-cced131a5c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n",
      "Evaluating Test Summaries: 100%|█████████████████████| 1431/1431 [03:31<00:00,  6.76it/s]\n",
      "Computing ROUGE Scores: 100%|████████████████████| 1431/1431 [00:00<00:00, 470158.94it/s]\n",
      "INFO:__main__:Average ROUGE Scores:\n",
      "INFO:__main__:rouge1: Precision=0.3691, Recall=0.1329, F1=0.1762\n",
      "INFO:__main__:rouge2: Precision=0.0483, Recall=0.0141, F1=0.0196\n",
      "INFO:__main__:rougeL: Precision=0.2755, Recall=0.0986, F1=0.1303\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Test and evaluate\n",
    "rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "rouge_scores = []\n",
    "output_file = 'test_abstractive_summaries.txt'\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"Test Summaries (Extractive and Abstractive)\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "\n",
    "    for chapter_id, extractive, reference in tqdm(test_abstractive_data, desc=\"Evaluating Test Summaries\"):\n",
    "        input_ids = [vocab_abstractive.get(token, vocab_abstractive['<UNK>']) for token in word_tokenize(extractive.lower())]\n",
    "        input_ids = [vocab_abstractive['<SOS>']] + input_ids + [vocab_abstractive['<EOS>']]\n",
    "        input_ids = input_ids[:400] + [vocab_abstractive['<PAD>']] * (400 - len(input_ids))\n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long).to(device)\n",
    "        generated_summary = model_abstractive.generate_summary(input_ids, vocab_abstractive, idx2word)\n",
    "        scores = rouge_scorer_instance.score(reference, generated_summary)\n",
    "        rouge_scores.append(scores)\n",
    "\n",
    "        # Write to file\n",
    "        f.write(f\"Chapter ID: {chapter_id}\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "        f.write(\"Extractive Summary:\\n\")\n",
    "        f.write(f\"{extractive}\\n\\n\")\n",
    "        f.write(\"Abstractive Summary:\\n\")\n",
    "        f.write(f\"{generated_summary}\\n\\n\")\n",
    "        f.write(\"Reference Summary:\\n\")\n",
    "        f.write(f\"{reference}\\n\\n\")\n",
    "        f.write(\"ROUGE Scores (Abstractive vs Reference):\\n\")\n",
    "        for metric in ['rouge1', 'rouge2', 'rougeL']:\n",
    "            f.write(f\"{metric.upper()}:\\n\")\n",
    "            f.write(f\"  Precision: {scores[metric].precision:.4f}\\n\")\n",
    "            f.write(f\"  Recall: {scores[metric].recall:.4f}\\n\")\n",
    "            f.write(f\"  F1: {scores[metric].fmeasure:.4f}\\n\")\n",
    "        f.write(\"\\n\" + \"=\" * 50 + \"\\n\\n\")\n",
    "\n",
    "avg_rouge = {\n",
    "    'rouge1': {'precision': 0, 'recall': 0, 'fmeasure': 0},\n",
    "    'rouge2': {'precision': 0, 'recall': 0, 'fmeasure': 0},\n",
    "    'rougeL': {'precision': 0, 'recall': 0, 'fmeasure': 0}\n",
    "}\n",
    "for scores in tqdm(rouge_scores, desc=\"Computing ROUGE Scores\"):\n",
    "    for metric in avg_rouge:\n",
    "        avg_rouge[metric]['precision'] += scores[metric].precision / len(rouge_scores)\n",
    "        avg_rouge[metric]['recall'] += scores[metric].recall / len(rouge_scores)\n",
    "        avg_rouge[metric]['fmeasure'] += scores[metric].fmeasure / len(rouge_scores)\n",
    "\n",
    "# Append average ROUGE scores to the file\n",
    "with open(output_file, 'a', encoding='utf-8') as f:\n",
    "    f.write(\"Average ROUGE Scores (Abstractive vs Reference)\\n\")\n",
    "    f.write(\"-\" * 50 + \"\\n\")\n",
    "    for metric, values in avg_rouge.items():\n",
    "        f.write(f\"{metric.upper()}:\\n\")\n",
    "        f.write(f\"  Precision: {values['precision']:.4f}\\n\")\n",
    "        f.write(f\"  Recall: {values['recall']:.4f}\\n\")\n",
    "        f.write(f\"  F1: {values['fmeasure']:.4f}\\n\")\n",
    "    f.write(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "logger.info(\"Average ROUGE Scores:\")\n",
    "for metric, values in avg_rouge.items():\n",
    "    logger.info(f\"{metric}: Precision={values['precision']:.4f}, Recall={values['recall']:.4f}, F1={values['fmeasure']:.4f}\")\n",
    "\n",
    "# Save abstractive model\n",
    "torch.save(model_abstractive.state_dict(), 'abstractive_model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

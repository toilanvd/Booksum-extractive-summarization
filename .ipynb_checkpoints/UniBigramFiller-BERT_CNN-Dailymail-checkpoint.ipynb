{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df16a5bf-b353-4b65-b6b6-ccf7710b25e3",
   "metadata": {},
   "source": [
    "# UniBigramFiller-BERT_CNN-Dailymail model for text summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d181ac9-c187-4a40-9693-a0118eab5b53",
   "metadata": {},
   "source": [
    "## Start of [1]: Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "179df10c-d582-4374-9dc7-529a4bb2af32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36db0d43-d3d1-4d87-9e43-efafa9f33bc5",
   "metadata": {},
   "source": [
    "Show training data head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "126813b5-0cfb-45c0-9857-d92cbaf52ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61df4979ac5fcc2b71be46ed6fe5a46ce7f071c3</td>\n",
       "      <td>Sally Forrest, an actress-dancer who graced the silver screen throughout the '40s and '50s in MGM musicals and films such as the 1956 noir While the City Sleeps died on March 15 at her home in Beverly Hills, California. Forrest, whose birth name was Katherine Feeney, was 86 and had long battled cancer. Her publicist, Judith Goffin, announced the news Thursday. Scroll down for video . Actress: Sally Forrest was in the 1951 Ida Lupino-directed film 'Hard, Fast and Beautiful' (left) and the 1956 Fritz Lang movie 'While the City Sleeps' A San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films including the critical and commercial success Not Wanted, Never Fear and Hard, Fast and Beautiful. Some of Forrest's other film credits included Bannerline, Son of Sinbad, and Excuse My Dust, according to her iMDB page. The page also indicates Forrest was in multiple Climax! and Rawhide television episodes. Forrest appeared as herself in an episode of The Ed Sullivan Show and three episodes of The Dinah Shore Chevy Show, her iMDB page says. She also starred in a Broadway production of The Seven Year Itch. City News Service reported that other stage credits included As You Like It, No, No, Nanette and Damn Yankees. Forrest married writer-producer Milo Frank in 1951. He died in 2004. She is survived by her niece, Sharon Durham, and nephews, Michael and Mark Feeney. Career: A San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films .</td>\n",
       "      <td>Sally Forrest, an actress-dancer who graced the silver screen throughout the '40s and '50s in MGM musicals and films died on March 15 .\\nForrest, whose birth name was Katherine Feeney, had long battled cancer .\\nA San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21c0bd69b7e7df285c3d1b1cf56d4da925980a68</td>\n",
       "      <td>A middle-school teacher in China has inked hundreds of sketches that are beyond be-leaf. Politics teacher Wang Lian, 35,  has created 1000 stunning ink drawings covering subjects as varied as cartoon characters and landscapes to animals, birds according to the People's Daily Online. The intricate scribbles on leaves feature Wang's favourite sites across the city of Nanjing, which include the Presidential Palace, Yangtze River Bridge, the ancient Jiming Temple and the Qinhuai River. Natural canvas: Artist and teacher Wang Lian has done hundreds of drawings, like this temple, on leaves she collects in the park and on the streets . Delicate: She uses an ink pen to gently draw the local scenes and buildings on the dried out leaves . 'Although teaching politics is my job, drawing is my passion and hobby,' said Wang. 'I first tried drawing on leaves about 10 years ago and fell in love with it as an art form immediately. 'It's like drawing on very old parchment paper, you have to be really careful that you don't damage the leaf because it is very fragile and this helps focus your attention and abilities.' Wang started giving the drawings away on Christmas Eve in 2012 when her junior high school son came home saying he wanted to prepare some gifts for his classmates. Being an avid painter, Wang decided to give her son's friends unique presents of gingko leaf paintings. Wang loves gingko leaves and will often pick them up along Gingko Avenue, near to her school, in Nanjing in east China's Jiangsu province. Every autumn she collects about 2,000 leaves from the ground to ensure she has enough to cover spoils too. Intricate: Teacher Wang has drawn hundreds of local scenes on leaves she has collected from the park . Hobby: The artist collects leaves every autumn and dries them out so she can sketch these impressive building scenes . 'The colour and shape of gingko leaves are particularly beautiful,' she said. 'I need to collect around 2000 leaves because this will include losses'. She takes them home where she then presses them between the pages of books. 'Luckily, I have quite a lot of books and I try to use old ones or ones that I've already read so I don't end up with nothing to read.' Once they are dried, she carefully takes each one and using an ink fountain pen creates her masterpieces. She said: 'Some people are into capturing beauty through photography, but for me, a digitalised image just isn't the same. New leaf: Politics teacher Wang Lian has drawn hundreds of doodles on leaves for the last 10 years . 'By drawing what I see I become far more a part of the process and part of the final piece. 'One day I hope to be able to put my collection on display, but for now it's really just for my own pleasure.' Wang's leaf paintings are turned into bookmarks, postcards and sometimes even given as gifts to her her students so she can share the beauty of leaf paintings. But locals who have had the luck of being able to see Wang's art have been gobsmacked. Local art collector On Hao, 58, said: 'These are truly remarkable and beautiful creations. 'She has so much talent she is wasted in teaching.'</td>\n",
       "      <td>Works include pictures of Presidential Palace and Yangtze River Bridge .\\nHas inked 1,000 pieces of art on leaves in last two years .\\nGives work away to students in form of bookmarks and postcards .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56f340189cd128194b2e7cb8c26bb900e3a848b4</td>\n",
       "      <td>A man convicted of killing the father and sister of his former girlfriend in a fiery attack on the family's Southern California home was sentenced to death on Tuesday. Iftekhar Murtaza, 30, was sentenced for the murders of Jay Dhanak, 56, and his daughter Karishma, 20, in May 2007, the Orange County district attorney's office said. Murtaza was convicted in December 2013 of killing the pair in an attempt to reunite with his then-18-year-old ex-girlfriend Shayona Dhanak. She had ended their relationship citing her Hindu family's opposition to her dating a Muslim. To be executed: Iftekhar Murtaza, 30, was sentenced to death Tuesday for the May 21, 2007 murders of his ex-girlfriend's father and sister and the attempted murder of her mother . Authorities said Murtaza and a friend torched the family's Anaheim Hills home and kidnapped and killed Dhanak's father and sister, leaving their stabbed bodies burning in a park 2 miles from Dhanak's dorm room at the University of California, Irvine. Dhanak's mother, Leela, survived the attack. She was stabbed and left unconscious on a neighbor's lawn. Murtaza was interviewed by police several days later and arrested at a Phoenix airport with a ticket to his native Bangladesh and more than $11,000 in cash. Jurors recommended that Murtaza be sentenced to death for the crimes. Attack: Murtaza torched his ex-girlfriend's family's Orange County home after they broke-up, believing the murders of her family would reunited them . Religious differences: Murtaza dated Shayona Dhanak when she was 18 in 2007. She broke up with him when her Hindu parents allegedly told her they would stop paying her college tuition if she continued to date the Muslim man . Two of his friends were also sentenced to life in prison for the murders, but one of them, Vitaliy Krasnoperov, recently had his conviction overturned on appeal. Authorities said Krasnoperov hatched the plot to kill the Dhanaks with Murtaza and tried to help him hire a hit man. They said another friend, Charles Murphy Jr., helped Murtaza carry out the killings after Dhanak said she planned to go on a date with someone else. During the trial, Murtaza testified that he told many people he wanted to kill the Dhanaks because he was distraught over the breakup, but he said he didn't mean it literally. Didn't do it alone: Two of Murtaza's friends have been convicted in connection to the killings . Killer: Leela Dhanak testified how Iftekhar Murtaza, seen in this August photo, murdered her husband and elder daughter in a failed attempt to win over her younger daughter . Bloodbath: Autopsy reports showed Jayprakash Dhanak (left) suffered 29 stab wounds to his body, while a pathologist testified that Karishma Dhanak (right) was alive when her throat was slit and her body set alight .</td>\n",
       "      <td>Iftekhar Murtaza, 29, was convicted a year ago of killing his ex-girlfriend's family in a fiery attack on the family home in 2007 .\\nOn Tuesday he was sentenced to death in Orange County .\\nFound guilty of stabbing Jaypraykash Dhanak, 56, slitting the throat of his 20-year-old daughter, Karishma, and setting their bodies on fire .\\nWife Leela Dhanak was stabbed in the stomach and had her throat slashed, but miraculously survived .\\nMurtaza concocted the murder plot with two friends after Dhanka's youngest daughter, Shayona, broke up with him over religious differences .\\nMurtaza married a 20-year-old suspected murderess in jail in 2011 after exchanging letters with her for five months .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00a665151b89a53e5a08a389df8334f4106494c2</td>\n",
       "      <td>Avid rugby fan Prince Harry could barely watch as England came up just short of the 26-point margin needed to win their first Six Nations since 2011 in a pulsating match at Twickenham. In a breathtaking spectacle, England defeated France 55-35 in 'Le Crunch' - just six points short of the total required to lift the trophy. Sporting a navy blue suit, the fourth-in-line to the throne squirmed in his seat as England got off to a difficult start, despite an early try. Scroll down for video . Stress: The Prince can barely watch as England beat France 55-35 but narrowly missed out on the Six Nations . Taking its toll: Harry looks anxious as England struggled to cope with France's impressive start to the match . Passion: Prince Harry belts out the national anthem, seated behind England coach Stuart Lancaster . Despite their herculean effort, England could not find a final try and finished second in the championship, behind Ireland. After the match, England head coach Stuart Lancaster praised his side for 'one of the most courageous performances' he has seen from his side. 'It will go down as one of the great games of rugby,' he added. Earlier in the day, Ireland thrashed 40-10 in Edinburgh to mark a dismal campaign for the Scots and an impressive Wales ran riot in Rome, thumping Italy 61-20. All smiles: The fourth-in-line to the throne grins alongside students from Reigate School, in Surrey . Before England's crunch match against France, Prince Harry had met girls from Reigate School and The Quest Academy, Croydon, who had played in the warm-up game. Harry - who is a Vice Patron of the Rugby Football Union (RFU) - also chatted with members of the armed forces at Twickenham . Looking dapper in his navy suit, the fourth-in-line to the throne then took his seat in the stand before singing the national anthem with gusto. Patriotic: Harry chats to a member of the army - he announced this week he will quit the army in the summer . Pointing the way: The Prince is an avid rugby fan - England and Wales host the World Cup later this year . Suave: Harry, in a sharp navy suit, walks along the side of the hallowed Twickenham turf before the match . The Six Nations is the last competitive rugby tournament before England host the World Cup later this year. The tournament begins on September 18 with England taking on Fiji at Twickenham. In 2014 Prince Harry was named as patron of the RFU's All Schools campaign, which aims to bring rugby to 750 more schools by the Rugby World Cup in 2019. Looking up: Prince Harry takes his seat in the stands with Bernard Lapasset, left, chairman of the IRB . Role model: Harry chats to students from Reigate School, who had played before England's match vs France . Charismatic: The Prince - patron of the RFU's All Schools campaign, jokes with the girls before a photo shoot . He is also Patron of the RFU Injured Players Foundation. Although one of the most prominent royal rugby fans (he was famously in attendance when England lifted the Webb Ellis Cup in Australia in 2003), the Prince is by no means the only one. His brother Prince William is also a fan and enjoys a similar position at the Welsh RFU while the Princess Royal is patron of Scottish rugby and regularly appears at Murrayfield on match day.</td>\n",
       "      <td>Prince Harry in attendance for England's crunch match against France .\\nHe met two girls' rugby teams and chatted with members of armed forces .\\nSporting a navy blue suit, the Prince belted out the national anthem .\\nEngland beat France 55-35 in pulsating match, but Ireland win Six Nations .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9f6fbd3c497c4d28879bebebea220884f03eb41a</td>\n",
       "      <td>A Triple M Radio producer has been inundated with messages from prospective partners after a workplace ploy. After Tuesday's Grill Team show, hosts Matty Johns, Mark Geyer and Gus Worland uploaded a picture of 26-year-old Nick Slater to Facebook with a mobile number where people could reach him. In less than 24 hours, he had received over 130 messages from a varied range of male and female listeners, reports News.com. Triple M producer Nick Slater, (C), pictured with his Grill Team hosts, was flooded with 130 voicemails in 24 hours . Workmates and Grill Team hosts Matty Johns, Mark Geyer and Gus Worland uploaded a picture of 26-year-old Nick Slater to Facebook with a mobile number where people could reach out . The ploy came about after a waitress handed the audio engineer her number while out at some work drinks. Unconvinced it was a one off, his colleagues decided to put it to the test and see if anyone else was romantically interested in him. 'The Producers had a few drinks on Friday &amp; Handsome Nick got a number off the waitress in the first 10 minutes!' 'We don't believe him that this never happens to him. Here's Nicks number...let's see how many calls he gets!' Slater received a torrent of voicemails, ranging from date proposals to 'heavy panting' 26-year-old Slater, a sound engineer, and his Triple M Grill Team workmates . In the following 24 hours Slater received a torrent of voicemails, ranging from date proposals to 'heavy panting.' 'There was a wide range of messages from some nice girls and a few nice blokes as well. There was even a grandma in there too.,' he said. 'We went through and listened to most of them. It was mostly people looking for lust, telling me to give them a call … it was all good fun.' Slater said his long-term girlfriend Kimberley understood of the station's antics, and she found the gag 'hilarious.' Slater said his long-term girlfriend Kimberley (pictured)  found the umber ploy 'hilarious .</td>\n",
       "      <td>Nick Slater's colleagues uploaded a picture to Facebook with his number .\\nIn less than 24 hours, he received over 130 messages from varied people .\\nSlater said his long-term girlfriend Kimberly found the ploy 'hilarious'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id  \\\n",
       "0  61df4979ac5fcc2b71be46ed6fe5a46ce7f071c3   \n",
       "1  21c0bd69b7e7df285c3d1b1cf56d4da925980a68   \n",
       "2  56f340189cd128194b2e7cb8c26bb900e3a848b4   \n",
       "3  00a665151b89a53e5a08a389df8334f4106494c2   \n",
       "4  9f6fbd3c497c4d28879bebebea220884f03eb41a   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               article  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Sally Forrest, an actress-dancer who graced the silver screen throughout the '40s and '50s in MGM musicals and films such as the 1956 noir While the City Sleeps died on March 15 at her home in Beverly Hills, California. Forrest, whose birth name was Katherine Feeney, was 86 and had long battled cancer. Her publicist, Judith Goffin, announced the news Thursday. Scroll down for video . Actress: Sally Forrest was in the 1951 Ida Lupino-directed film 'Hard, Fast and Beautiful' (left) and the 1956 Fritz Lang movie 'While the City Sleeps' A San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films including the critical and commercial success Not Wanted, Never Fear and Hard, Fast and Beautiful. Some of Forrest's other film credits included Bannerline, Son of Sinbad, and Excuse My Dust, according to her iMDB page. The page also indicates Forrest was in multiple Climax! and Rawhide television episodes. Forrest appeared as herself in an episode of The Ed Sullivan Show and three episodes of The Dinah Shore Chevy Show, her iMDB page says. She also starred in a Broadway production of The Seven Year Itch. City News Service reported that other stage credits included As You Like It, No, No, Nanette and Damn Yankees. Forrest married writer-producer Milo Frank in 1951. He died in 2004. She is survived by her niece, Sharon Durham, and nephews, Michael and Mark Feeney. Career: A San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films .   \n",
       "1                                                                                                                                    A middle-school teacher in China has inked hundreds of sketches that are beyond be-leaf. Politics teacher Wang Lian, 35,  has created 1000 stunning ink drawings covering subjects as varied as cartoon characters and landscapes to animals, birds according to the People's Daily Online. The intricate scribbles on leaves feature Wang's favourite sites across the city of Nanjing, which include the Presidential Palace, Yangtze River Bridge, the ancient Jiming Temple and the Qinhuai River. Natural canvas: Artist and teacher Wang Lian has done hundreds of drawings, like this temple, on leaves she collects in the park and on the streets . Delicate: She uses an ink pen to gently draw the local scenes and buildings on the dried out leaves . 'Although teaching politics is my job, drawing is my passion and hobby,' said Wang. 'I first tried drawing on leaves about 10 years ago and fell in love with it as an art form immediately. 'It's like drawing on very old parchment paper, you have to be really careful that you don't damage the leaf because it is very fragile and this helps focus your attention and abilities.' Wang started giving the drawings away on Christmas Eve in 2012 when her junior high school son came home saying he wanted to prepare some gifts for his classmates. Being an avid painter, Wang decided to give her son's friends unique presents of gingko leaf paintings. Wang loves gingko leaves and will often pick them up along Gingko Avenue, near to her school, in Nanjing in east China's Jiangsu province. Every autumn she collects about 2,000 leaves from the ground to ensure she has enough to cover spoils too. Intricate: Teacher Wang has drawn hundreds of local scenes on leaves she has collected from the park . Hobby: The artist collects leaves every autumn and dries them out so she can sketch these impressive building scenes . 'The colour and shape of gingko leaves are particularly beautiful,' she said. 'I need to collect around 2000 leaves because this will include losses'. She takes them home where she then presses them between the pages of books. 'Luckily, I have quite a lot of books and I try to use old ones or ones that I've already read so I don't end up with nothing to read.' Once they are dried, she carefully takes each one and using an ink fountain pen creates her masterpieces. She said: 'Some people are into capturing beauty through photography, but for me, a digitalised image just isn't the same. New leaf: Politics teacher Wang Lian has drawn hundreds of doodles on leaves for the last 10 years . 'By drawing what I see I become far more a part of the process and part of the final piece. 'One day I hope to be able to put my collection on display, but for now it's really just for my own pleasure.' Wang's leaf paintings are turned into bookmarks, postcards and sometimes even given as gifts to her her students so she can share the beauty of leaf paintings. But locals who have had the luck of being able to see Wang's art have been gobsmacked. Local art collector On Hao, 58, said: 'These are truly remarkable and beautiful creations. 'She has so much talent she is wasted in teaching.'   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       A man convicted of killing the father and sister of his former girlfriend in a fiery attack on the family's Southern California home was sentenced to death on Tuesday. Iftekhar Murtaza, 30, was sentenced for the murders of Jay Dhanak, 56, and his daughter Karishma, 20, in May 2007, the Orange County district attorney's office said. Murtaza was convicted in December 2013 of killing the pair in an attempt to reunite with his then-18-year-old ex-girlfriend Shayona Dhanak. She had ended their relationship citing her Hindu family's opposition to her dating a Muslim. To be executed: Iftekhar Murtaza, 30, was sentenced to death Tuesday for the May 21, 2007 murders of his ex-girlfriend's father and sister and the attempted murder of her mother . Authorities said Murtaza and a friend torched the family's Anaheim Hills home and kidnapped and killed Dhanak's father and sister, leaving their stabbed bodies burning in a park 2 miles from Dhanak's dorm room at the University of California, Irvine. Dhanak's mother, Leela, survived the attack. She was stabbed and left unconscious on a neighbor's lawn. Murtaza was interviewed by police several days later and arrested at a Phoenix airport with a ticket to his native Bangladesh and more than $11,000 in cash. Jurors recommended that Murtaza be sentenced to death for the crimes. Attack: Murtaza torched his ex-girlfriend's family's Orange County home after they broke-up, believing the murders of her family would reunited them . Religious differences: Murtaza dated Shayona Dhanak when she was 18 in 2007. She broke up with him when her Hindu parents allegedly told her they would stop paying her college tuition if she continued to date the Muslim man . Two of his friends were also sentenced to life in prison for the murders, but one of them, Vitaliy Krasnoperov, recently had his conviction overturned on appeal. Authorities said Krasnoperov hatched the plot to kill the Dhanaks with Murtaza and tried to help him hire a hit man. They said another friend, Charles Murphy Jr., helped Murtaza carry out the killings after Dhanak said she planned to go on a date with someone else. During the trial, Murtaza testified that he told many people he wanted to kill the Dhanaks because he was distraught over the breakup, but he said he didn't mean it literally. Didn't do it alone: Two of Murtaza's friends have been convicted in connection to the killings . Killer: Leela Dhanak testified how Iftekhar Murtaza, seen in this August photo, murdered her husband and elder daughter in a failed attempt to win over her younger daughter . Bloodbath: Autopsy reports showed Jayprakash Dhanak (left) suffered 29 stab wounds to his body, while a pathologist testified that Karishma Dhanak (right) was alive when her throat was slit and her body set alight .   \n",
       "3  Avid rugby fan Prince Harry could barely watch as England came up just short of the 26-point margin needed to win their first Six Nations since 2011 in a pulsating match at Twickenham. In a breathtaking spectacle, England defeated France 55-35 in 'Le Crunch' - just six points short of the total required to lift the trophy. Sporting a navy blue suit, the fourth-in-line to the throne squirmed in his seat as England got off to a difficult start, despite an early try. Scroll down for video . Stress: The Prince can barely watch as England beat France 55-35 but narrowly missed out on the Six Nations . Taking its toll: Harry looks anxious as England struggled to cope with France's impressive start to the match . Passion: Prince Harry belts out the national anthem, seated behind England coach Stuart Lancaster . Despite their herculean effort, England could not find a final try and finished second in the championship, behind Ireland. After the match, England head coach Stuart Lancaster praised his side for 'one of the most courageous performances' he has seen from his side. 'It will go down as one of the great games of rugby,' he added. Earlier in the day, Ireland thrashed 40-10 in Edinburgh to mark a dismal campaign for the Scots and an impressive Wales ran riot in Rome, thumping Italy 61-20. All smiles: The fourth-in-line to the throne grins alongside students from Reigate School, in Surrey . Before England's crunch match against France, Prince Harry had met girls from Reigate School and The Quest Academy, Croydon, who had played in the warm-up game. Harry - who is a Vice Patron of the Rugby Football Union (RFU) - also chatted with members of the armed forces at Twickenham . Looking dapper in his navy suit, the fourth-in-line to the throne then took his seat in the stand before singing the national anthem with gusto. Patriotic: Harry chats to a member of the army - he announced this week he will quit the army in the summer . Pointing the way: The Prince is an avid rugby fan - England and Wales host the World Cup later this year . Suave: Harry, in a sharp navy suit, walks along the side of the hallowed Twickenham turf before the match . The Six Nations is the last competitive rugby tournament before England host the World Cup later this year. The tournament begins on September 18 with England taking on Fiji at Twickenham. In 2014 Prince Harry was named as patron of the RFU's All Schools campaign, which aims to bring rugby to 750 more schools by the Rugby World Cup in 2019. Looking up: Prince Harry takes his seat in the stands with Bernard Lapasset, left, chairman of the IRB . Role model: Harry chats to students from Reigate School, who had played before England's match vs France . Charismatic: The Prince - patron of the RFU's All Schools campaign, jokes with the girls before a photo shoot . He is also Patron of the RFU Injured Players Foundation. Although one of the most prominent royal rugby fans (he was famously in attendance when England lifted the Webb Ellis Cup in Australia in 2003), the Prince is by no means the only one. His brother Prince William is also a fan and enjoys a similar position at the Welsh RFU while the Princess Royal is patron of Scottish rugby and regularly appears at Murrayfield on match day.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  A Triple M Radio producer has been inundated with messages from prospective partners after a workplace ploy. After Tuesday's Grill Team show, hosts Matty Johns, Mark Geyer and Gus Worland uploaded a picture of 26-year-old Nick Slater to Facebook with a mobile number where people could reach him. In less than 24 hours, he had received over 130 messages from a varied range of male and female listeners, reports News.com. Triple M producer Nick Slater, (C), pictured with his Grill Team hosts, was flooded with 130 voicemails in 24 hours . Workmates and Grill Team hosts Matty Johns, Mark Geyer and Gus Worland uploaded a picture of 26-year-old Nick Slater to Facebook with a mobile number where people could reach out . The ploy came about after a waitress handed the audio engineer her number while out at some work drinks. Unconvinced it was a one off, his colleagues decided to put it to the test and see if anyone else was romantically interested in him. 'The Producers had a few drinks on Friday & Handsome Nick got a number off the waitress in the first 10 minutes!' 'We don't believe him that this never happens to him. Here's Nicks number...let's see how many calls he gets!' Slater received a torrent of voicemails, ranging from date proposals to 'heavy panting' 26-year-old Slater, a sound engineer, and his Triple M Grill Team workmates . In the following 24 hours Slater received a torrent of voicemails, ranging from date proposals to 'heavy panting.' 'There was a wide range of messages from some nice girls and a few nice blokes as well. There was even a grandma in there too.,' he said. 'We went through and listened to most of them. It was mostly people looking for lust, telling me to give them a call … it was all good fun.' Slater said his long-term girlfriend Kimberley understood of the station's antics, and she found the gag 'hilarious.' Slater said his long-term girlfriend Kimberley (pictured)  found the umber ploy 'hilarious .   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                highlights  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                          Sally Forrest, an actress-dancer who graced the silver screen throughout the '40s and '50s in MGM musicals and films died on March 15 .\\nForrest, whose birth name was Katherine Feeney, had long battled cancer .\\nA San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films .  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Works include pictures of Presidential Palace and Yangtze River Bridge .\\nHas inked 1,000 pieces of art on leaves in last two years .\\nGives work away to students in form of bookmarks and postcards .  \n",
       "2  Iftekhar Murtaza, 29, was convicted a year ago of killing his ex-girlfriend's family in a fiery attack on the family home in 2007 .\\nOn Tuesday he was sentenced to death in Orange County .\\nFound guilty of stabbing Jaypraykash Dhanak, 56, slitting the throat of his 20-year-old daughter, Karishma, and setting their bodies on fire .\\nWife Leela Dhanak was stabbed in the stomach and had her throat slashed, but miraculously survived .\\nMurtaza concocted the murder plot with two friends after Dhanka's youngest daughter, Shayona, broke up with him over religious differences .\\nMurtaza married a 20-year-old suspected murderess in jail in 2011 after exchanging letters with her for five months .  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                    Prince Harry in attendance for England's crunch match against France .\\nHe met two girls' rugby teams and chatted with members of armed forces .\\nSporting a navy blue suit, the Prince belted out the national anthem .\\nEngland beat France 55-35 in pulsating match, but Ireland win Six Nations .  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Nick Slater's colleagues uploaded a picture to Facebook with his number .\\nIn less than 24 hours, he received over 130 messages from varied people .\\nSlater said his long-term girlfriend Kimberly found the ploy 'hilarious'  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "val = pd.read_csv(\"Dataset/cnn_dailymail/validation.csv\")\n",
    "test = pd.read_csv(\"Dataset/cnn_dailymail/test.csv\")\n",
    "val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac330a27-a92b-40f1-bbfe-1de78ba57ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sally Forrest, an actress-dancer who graced the silver screen throughout the '40s and '50s in MGM musicals and films such as the 1956 noir While the City Sleeps died on March 15 at her home in Beverly Hills, California. Forrest, whose birth name was Katherine Feeney, was 86 and had long battled cancer. Her publicist, Judith Goffin, announced the news Thursday. Scroll down for video . Actress: Sally Forrest was in the 1951 Ida Lupino-directed film 'Hard, Fast and Beautiful' (left) and the 1956 Fritz Lang movie 'While the City Sleeps' A San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films including the critical and commercial success Not Wanted, Never Fear and Hard, Fast and Beautiful. Some of Forrest's other film credits included Bannerline, Son of Sinbad, and Excuse My Dust, according to her iMDB page. The page also indicates Forrest was in multiple Climax! and Rawhide television episodes. Forrest appeared as herself in an episode of The Ed Sullivan Show and three episodes of The Dinah Shore Chevy Show, her iMDB page says. She also starred in a Broadway production of The Seven Year Itch. City News Service reported that other stage credits included As You Like It, No, No, Nanette and Damn Yankees. Forrest married writer-producer Milo Frank in 1951. He died in 2004. She is survived by her niece, Sharon Durham, and nephews, Michael and Mark Feeney. Career: A San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films .\n"
     ]
    }
   ],
   "source": [
    "print(val.iloc[0]['article'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e638c12c-2e6c-40f3-8b5e-75839ca216d3",
   "metadata": {},
   "source": [
    "## End of [1]: Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310dbb9f-a741-4c49-8b05-414a1ff8dcbc",
   "metadata": {},
   "source": [
    "## Start of [2]: Span recognition (sentence splitting in this model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff9d47bb-3b15-4a11-a131-cd057934a4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/toilanvd/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad797c05-ed8f-45f3-8ea2-64fef1d01113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello!', 'Hello world...', \"It's true that Mr.Ryo is here?\", \"That's true!\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "sent_tokenize(\"Hello! Hello world... It's true that Mr.Ryo is here? That's true!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d16969b-79b5-4c2b-aa9d-2488207a4f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Sally Forrest, an actress-dancer who graced the silver screen throughout the '40s and '50s in MGM musicals and films such as the 1956 noir While the City Sleeps died on March 15 at her home in Beverly Hills, California. Forrest, whose birth name was Katherine Feeney, was 86 and had long battled cancer. Her publicist, Judith Goffin, announced the news Thursday. Scroll down for video . Actress: Sally Forrest was in the 1951 Ida Lupino-directed film 'Hard, Fast and Beautiful' (left) and the 1956 Fritz Lang movie 'While the City Sleeps' A San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films including the critical and commercial success Not Wanted, Never Fear and Hard, Fast and Beautiful. Some of Forrest's other film credits included Bannerline, Son of Sinbad, and Excuse My Dust, according to her iMDB\\xa0page. The page also indicates Forrest was in multiple Climax! and Rawhide television episodes. Forrest appeared as herself in an episode of The Ed Sullivan Show and three episodes of The Dinah Shore Chevy Show, her iMDB page says. She also starred in a Broadway production of The Seven Year Itch. City News Service reported that other stage credits included As You Like It, No, No, Nanette and Damn Yankees. Forrest married writer-producer Milo Frank in 1951. He died in 2004. She is survived by her niece, Sharon Durham, and nephews, Michael and Mark Feeney. Career: A San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films .\"]\n"
     ]
    }
   ],
   "source": [
    "val_chapters = val['article'].tolist()\n",
    "val_chapters_paragraphs = [val_chapters[i].split(\"\\n\\n\") for i in range(len(val_chapters))]\n",
    "for i in range(len(val_chapters_paragraphs)):\n",
    "    val_chapters_paragraphs[i] = [val_chapters_paragraphs[i][j].replace(\"\\n\", \" \") for j in range(len(val_chapters_paragraphs[i]))]\n",
    "val_chapters = [val_chapters[i].replace(\"\\n\", \" \") for i in range(len(val_chapters))]\n",
    "test_chapters = test['article'].tolist()\n",
    "test_chapters_paragraphs = [test_chapters[i].split(\"\\n\\n\") for i in range(len(test_chapters))]\n",
    "for i in range(len(test_chapters_paragraphs)):\n",
    "    test_chapters_paragraphs[i] = [test_chapters_paragraphs[i][j].replace(\"\\n\", \" \") for j in range(len(test_chapters_paragraphs[i]))]\n",
    "test_chapters = [test_chapters[i].replace(\"\\n\", \" \") for i in range(len(test_chapters))]\n",
    "\n",
    "print(val_chapters_paragraphs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b145b468-a7ea-4d8a-b76e-8e5147860cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"Sally Forrest, an actress-dancer who graced the silver screen throughout the '40s and '50s in MGM musicals and films such as the 1956 noir While the City Sleeps died on March 15 at her home in Beverly Hills, California.\", 'Forrest, whose birth name was Katherine Feeney, was 86 and had long battled cancer.', 'Her publicist, Judith Goffin, announced the news Thursday.', 'Scroll down for video .', \"Actress: Sally Forrest was in the 1951 Ida Lupino-directed film 'Hard, Fast and Beautiful' (left) and the 1956 Fritz Lang movie 'While the City Sleeps' A San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films including the critical and commercial success Not Wanted, Never Fear and Hard, Fast and Beautiful.\", \"Some of Forrest's other film credits included Bannerline, Son of Sinbad, and Excuse My Dust, according to her iMDB\\xa0page.\", 'The page also indicates Forrest was in multiple Climax!', 'and Rawhide television episodes.', 'Forrest appeared as herself in an episode of The Ed Sullivan Show and three episodes of The Dinah Shore Chevy Show, her iMDB page says.', 'She also starred in a Broadway production of The Seven Year Itch.', 'City News Service reported that other stage credits included As You Like It, No, No, Nanette and Damn Yankees.', 'Forrest married writer-producer Milo Frank in 1951.', 'He died in 2004.', 'She is survived by her niece, Sharon Durham, and nephews, Michael and Mark Feeney.', 'Career: A San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films .']]\n"
     ]
    }
   ],
   "source": [
    "val_chapters_sentences = [sent_tokenize(val_chapters[i]) for i in range(len(val_chapters))]\n",
    "test_chapters_sentences = [sent_tokenize(test_chapters[i]) for i in range(len(test_chapters))]\n",
    "\n",
    "val_chapters_paragraphs_sentences = []\n",
    "for i in range(len(val_chapters_paragraphs)): # chapter i\n",
    "    chapter_list = []\n",
    "    for j in range(len(val_chapters_paragraphs[i])): # paragraph j\n",
    "        chapter_list.append(sent_tokenize(val_chapters_paragraphs[i][j]))\n",
    "    val_chapters_paragraphs_sentences.append(chapter_list)\n",
    "test_chapters_paragraphs_sentences = []\n",
    "for i in range(len(test_chapters_paragraphs)): # chapter i\n",
    "    chapter_list = []\n",
    "    for j in range(len(test_chapters_paragraphs[i])): # paragraph j\n",
    "        chapter_list.append(sent_tokenize(test_chapters_paragraphs[i][j]))\n",
    "    test_chapters_paragraphs_sentences.append(chapter_list)\n",
    "\n",
    "print(val_chapters_paragraphs_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9083aeb-54c5-4ccf-96e1-bfdfd6c4f1f3",
   "metadata": {},
   "source": [
    "## End of [2]: Span recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bb0b93-563f-4abc-827d-cccc663cf5b9",
   "metadata": {},
   "source": [
    "## Start of [3]: (BERT + cosine) score function for sentence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b462624b-194e-4b1d-b98a-18e07b93c53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Her publicist, Judith Goffin, announced the news Thursday.\n",
      "Scroll down for video .\n",
      "0.03955427557229996\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "sentence_embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2', device = device)  # multi-language model  \n",
    "\n",
    "def sentence_similarity(sentence_A, sentence_B):\n",
    "    temp_sentence_A = [sentence_A]\n",
    "    temp_embedding_A = sentence_embedding_model.encode(temp_sentence_A, convert_to_tensor=False)\n",
    "    temp_sentence_B = [sentence_B]\n",
    "    temp_embedding_B = sentence_embedding_model.encode(temp_sentence_B, convert_to_tensor=False)\n",
    "    cosine_score = util.cos_sim(temp_embedding_A, temp_embedding_B)\n",
    "    return cosine_score[0][0].item()\n",
    "\n",
    "print(val_chapters_sentences[0][2])\n",
    "print(val_chapters_sentences[0][3])\n",
    "print(sentence_similarity(val_chapters_sentences[0][2], val_chapters_sentences[0][3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09c052a-85b1-4f60-b13c-16b8bc851a34",
   "metadata": {},
   "source": [
    "## End of [3]: (BERT + cosine) score function for sentence similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50387011-e37e-4d2a-afc2-3fc0bcdf876c",
   "metadata": {},
   "source": [
    "## Start of [5']: Metrics preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc37c500-4242-4b80-b3a0-6ad33e7c5353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation reference summaries\n",
    "val_reference_summaries = val['highlights'].tolist()\n",
    "val_reference_summaries = [val_reference_summaries[i].replace(\"\\n\", \" \") for i in range(len(val_reference_summaries))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbd7a6a8-9021-4e9c-a641-e832b2566c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09802262149041631\n"
     ]
    }
   ],
   "source": [
    "# Calculate average summary length vs chapter length ratio\n",
    "average_summary_length_ratio = 0\n",
    "for i in range(len(val_chapters)):\n",
    "    average_summary_length_ratio += len(word_tokenize(val_reference_summaries[i])) / len(word_tokenize(val_chapters[i]))\n",
    "average_summary_length_ratio /= len(val_chapters)\n",
    "#average_summary_length_ratio = 0.1\n",
    "print(average_summary_length_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81759ca9-43f8-4f76-a4b6-e9eb0a673c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "EPS = 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a413c054-9277-4569-a2ec-a6e6b7728660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "rougescorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1567d40e-7411-456d-bfd9-c821116aa93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': Score(precision=1.0, recall=0.8333333333333334, fmeasure=0.9090909090909091), 'rouge2': Score(precision=0.25, recall=0.2, fmeasure=0.22222222222222224), 'rougeL': Score(precision=0.8, recall=0.6666666666666666, fmeasure=0.7272727272727272)}\n"
     ]
    }
   ],
   "source": [
    "sent_A = \"1 I love machine learning\"\n",
    "sent_B = \"1 think love i machine learning..\"\n",
    "print(rougescorer.score(sent_B, sent_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9f8383d-de4e-4e83-8bde-630c17e6cf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import bert_score\n",
    "from bert_score import BERTScorer\n",
    "\n",
    "bertscorer = BERTScorer(lang='en', rescale_with_baseline=True) # Default using 'roberta-large' model\n",
    "#bertscorer = BERTScorer(model_type='xlnet-large-cased', rescale_with_baseline=True, lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8928a855-f56d-4b51-ad69-b9fc422f7c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toilanvd/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from summaqa import evaluate_corpus\n",
    "from summaqa import QG_masked\n",
    "question_generator = QG_masked()\n",
    "from summaqa import QA_Metric\n",
    "qa_metric = QA_Metric()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d104c54-16c0-4c8f-81b8-a9d064bc0f67",
   "metadata": {},
   "source": [
    "## End of [5']: Metrics preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dde843-5d32-4e0e-a37e-4c1e216b8429",
   "metadata": {},
   "source": [
    "## Start of [6']: Run test & evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ec7114c-12c8-48ad-b2df-dad75bd242d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reference summaries\n",
    "test_reference_summaries = test['highlights'].tolist()\n",
    "test_reference_summaries = [test_reference_summaries[i].replace(\"\\n\", \" \") for i in range(len(test_reference_summaries))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0d999411-d0c4-4c56-ab13-71f4158d50cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test doc 0: 16 sentences\n",
      "Test doc 1: 14 sentences\n",
      "Test doc 2: 6 sentences\n",
      "Test doc 3: 15 sentences\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m         bigram \u001b[38;5;241m=\u001b[39m doc_words[j] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m doc_words[j \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     17\u001b[0m         gram_list\u001b[38;5;241m.\u001b[39mappend(bigram)\n\u001b[0;32m---> 18\u001b[0m grams_emb \u001b[38;5;241m=\u001b[39m \u001b[43msentence_embedding_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgram_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m grams_importance \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mcos_sim(doc_emb, grams_emb)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     20\u001b[0m gram_list_to_compare \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:165\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    162\u001b[0m features \u001b[38;5;241m=\u001b[39m batch_to_device(features, device)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 165\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    168\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:66\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m     64\u001b[0m     trans_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 66\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     69\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m: output_tokens, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1013\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1015\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1016\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1017\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[0;32m-> 1022\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1035\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:612\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    603\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    604\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    605\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    609\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    610\u001b[0m     )\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 612\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    487\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    419\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 427\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:365\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    363\u001b[0m     attention_probs \u001b[38;5;241m=\u001b[39m attention_probs \u001b[38;5;241m*\u001b[39m head_mask\n\u001b[0;32m--> 365\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    368\u001b[0m new_context_layer_shape \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size,)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test summary generation\n",
    "test_generated_summaries = []\n",
    "for i in range(len(test_chapters)):\n",
    "    num_sentences = len(test_chapters_sentences[i])\n",
    "    print(\"Test doc {}: {} sentences\".format(i, len(test_chapters_sentences[i])))\n",
    "    summary_length_limit = math.ceil(len(word_tokenize(test_chapters[i])) * average_summary_length_ratio)\n",
    "    #summary_length_limit = sum(sentences_length)\n",
    "\n",
    "    doc_emb = sum(sentence_embedding_model.encode(test_chapters_sentences[i], convert_to_tensor=False))\n",
    "    doc_words = word_tokenize(test_chapters[i])\n",
    "    gram_list = []\n",
    "    for j in range(len(doc_words)):\n",
    "        unigram = doc_words[j]\n",
    "        gram_list.append(unigram)\n",
    "        if j < len(doc_words) - 1:\n",
    "            bigram = doc_words[j] + ' ' + doc_words[j + 1]\n",
    "            gram_list.append(bigram)\n",
    "    grams_emb = sentence_embedding_model.encode(gram_list, convert_to_tensor=False)\n",
    "    grams_importance = util.cos_sim(doc_emb, grams_emb)[0]\n",
    "    gram_list_to_compare = []\n",
    "    for j in range(len(gram_list)):\n",
    "        gram_list_to_compare.append((grams_importance[j].item(), j))\n",
    "    gram_list_to_compare.sort(reverse = True)\n",
    "    \n",
    "    inputFile = open(\"UniBigramFiller-BERT-input.txt\", \"w\")\n",
    "    print(\"{}\".format(test_chapters[i]), file = inputFile)\n",
    "    print(\"{}\".format(summary_length_limit), file = inputFile)\n",
    "    for j in range(len(gram_list)):\n",
    "        print(\"{0:.9f} \".format(grams_importance[gram_list_to_compare[j][1]].item()), end = \"\", file = inputFile)\n",
    "        print(\"{}\".format(gram_list[gram_list_to_compare[j][1]]), file = inputFile)\n",
    "    inputFile.close()\n",
    "    \n",
    "    os.system(\"./UniBigramFiller-BERT\")\n",
    "    outputFile = open(\"UniBigramFiller-BERT-output.txt\", \"r\")\n",
    "    generated_summary = outputFile.readline()\n",
    "    current_profit = float(outputFile.readline())\n",
    "    print(\"Profit = {0:.9f}\".format(current_profit))\n",
    "    outputFile.close()\n",
    "    \n",
    "    test_generated_summaries.append(generated_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8027baa9-946c-4575-87ee-8046b6aba325",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_generated_summaries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4d1ee9-f644-486b-bf11-2acb8eb2963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ROUGE score\n",
    "test_rouge1 = 0\n",
    "test_rouge2 = 0\n",
    "test_rougeL = 0\n",
    "for i in range(len(test_generated_summaries)):\n",
    "    print(i)\n",
    "    scores = rougescorer.score(test_reference_summaries[i], test_generated_summaries[i])\n",
    "    for key in scores:\n",
    "        print(\"{}: {}\".format(key, scores[key]))\n",
    "        if key == \"rouge1\":\n",
    "            test_rouge1 += scores[key][2] # take fmeasure value\n",
    "        elif key == \"rouge2\":\n",
    "            test_rouge2 += scores[key][2] # take fmeasure value\n",
    "        else:\n",
    "            test_rougeL += scores[key][2] # take fmeasure value\n",
    "test_rouge1 /= len(test_generated_summaries)\n",
    "test_rouge2 /= len(test_generated_summaries)\n",
    "test_rougeL /= len(test_generated_summaries)\n",
    "print(\"Test: rouge1 = {}, rouge2 = {}, rougeL = {}\".format(test_rouge1, test_rouge2, test_rougeL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318a221a-1cd4-43be-9785-ac13497862aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print test result to file\n",
    "test_result_file = open(\"UniBigramFiller-BERT_CNN-Dailymail_test-generated-result.txt\", \"w\")\n",
    "for i in range(len(test_generated_summaries)):\n",
    "    print(test_generated_summaries[i], file = test_result_file)\n",
    "test_result_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aa2d64-9419-4245-920e-52d8267685a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test BERT score\n",
    "average_bertscore_F1 = 0\n",
    "\n",
    "for i in range(len(test_generated_summaries)):\n",
    "    print(\"Test doc {}:\".format(i))\n",
    "    \n",
    "    ref_summary = copy.deepcopy(test_reference_summaries[i])\n",
    "    candidate_summary = copy.deepcopy(test_generated_summaries[i])\n",
    "\n",
    "    # Method 1: Split by sentence\n",
    "    ref_sentences = sent_tokenize(ref_summary)\n",
    "    candidate_sentences = sent_tokenize(candidate_summary)\n",
    "    chapter_bertscore_F1 = 0\n",
    "    for j in range(len(ref_sentences)):\n",
    "        ref_summary_split = ref_sentences[j]\n",
    "        max_candidate_score = -1\n",
    "        for k in range(len(candidate_sentences)):\n",
    "            candidate_summary_split = candidate_sentences[k]\n",
    "            P, R, F1 = bertscorer.score([candidate_summary_split], [ref_summary_split])\n",
    "            #print(\"Candidate length = {}, reference length = {}\".format(len(word_tokenize(candidate_summary_split)), len(word_tokenize(ref_summary_split))))\n",
    "            max_candidate_score = max(max_candidate_score, F1.mean().item())\n",
    "        chapter_bertscore_F1 += max_candidate_score\n",
    "    chapter_bertscore_F1 /= len(ref_sentences)\n",
    "    average_bertscore_F1 += chapter_bertscore_F1\n",
    "    print(\"Chapter BERTScore F1 = {}\".format(chapter_bertscore_F1))\n",
    "\n",
    "    '''\n",
    "    # Method 2: Split by n words\n",
    "    ref_tokenized = word_tokenize(ref_summary)\n",
    "    candidate_tokenized = word_tokenize(candidate_summary)\n",
    "    split_length = 250\n",
    "    ref_pointer = 0\n",
    "    ref_split_count = 0\n",
    "    chapter_bertscore_F1 = 0\n",
    "    while ref_pointer < len(ref_tokenized):\n",
    "        ref_summary_split = ' '.join(ref_tokenized[ref_pointer:min(len(ref_tokenized), ref_pointer + split_length)])\n",
    "        max_candidate_score = -1\n",
    "        candidate_pointer = 0\n",
    "        while candidate_pointer < len(candidate_tokenized):\n",
    "            candidate_summary_split = ' '.join(candidate_tokenized[candidate_pointer:min(len(candidate_tokenized), candidate_pointer + split_length)])\n",
    "            P, R, F1 = bertscorer.score([candidate_summary_split], [ref_summary_split])\n",
    "            #print(\"Candidate length = {}, reference length = {}\".format(len(word_tokenize(candidate_summary_split)), len(word_tokenize(ref_summary_split))))\n",
    "            max_candidate_score = max(max_candidate_score, F1.mean().item())\n",
    "            candidate_pointer += split_length\n",
    "        chapter_bertscore_F1 += max_candidate_score\n",
    "        ref_pointer += split_length\n",
    "        ref_split_count += 1\n",
    "    chapter_bertscore_F1 /= ref_split_count\n",
    "    average_bertscore_F1 += chapter_bertscore_F1\n",
    "    print(\"Chapter BERTScore F1 = {}\".format(chapter_bertscore_F1))\n",
    "    '''\n",
    "average_bertscore_F1 /= len(test_generated_summaries)\n",
    "print(\"Test average BERTscore F1 = {}\".format(average_bertscore_F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a9feed-dcea-4e27-b01b-db97fb93e76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test summaQA\n",
    "#srcs = test_chapters[:len(test_generated_summaries)]\n",
    "#gens = test_generated_summaries\n",
    "#srcs = [' '.join(' '.join(srcs[i].split(\" \")).split()[:300]) for i in range(len(srcs))]\n",
    "#gens = [' '.join(' '.join(gens[i].split(\" \")).split()[:300]) for i in range(len(gens))]\n",
    "#evaluate_corpus(srcs, gens)\n",
    "\n",
    "average_summaqa_prob = 0\n",
    "average_summaqa_F1 = 0\n",
    "\n",
    "for i in range(len(test_generated_summaries)):\n",
    "    print(\"Test doc {}:\".format(i))\n",
    "    \n",
    "    chapter_content = copy.deepcopy(test_chapters[i])\n",
    "    candidate_summary = copy.deepcopy(test_generated_summaries[i])\n",
    "\n",
    "    # Method 1: Split by sentence\n",
    "    chapter_sentences = test_chapters_sentences[i]\n",
    "    candidate_sentences = sent_tokenize(candidate_summary)\n",
    "    chapter_split_count = 0\n",
    "    chapter_summaqa_prob = 0\n",
    "    chapter_summaqa_F1 = 0\n",
    "    for j in range(len(chapter_sentences)):\n",
    "        chapter_summary_split = chapter_sentences[j]\n",
    "        article = chapter_summary_split\n",
    "        masked_questions, answer_spans = question_generator.get_questions(article)\n",
    "        if len(masked_questions) == 0:\n",
    "            continue\n",
    "        chapter_split_count += 1\n",
    "        print(\"Chapter split #{}:\".format(j))\n",
    "        print(chapter_summary_split)\n",
    "        print(\"Questions:\")\n",
    "        print(masked_questions)\n",
    "        print(\"Answers:\")\n",
    "        print(answer_spans)\n",
    "        max_candidate_F1 = 0\n",
    "        max_candidate_prob = 0\n",
    "        #chapter_split_sum_F1 = 0\n",
    "        #chapter_split_sum_prob = 0\n",
    "        for k in range(len(candidate_sentences)):\n",
    "            candidate_summary_split = candidate_sentences[k]\n",
    "            summaqa_scores = qa_metric.compute(masked_questions, answer_spans, candidate_summary_split)\n",
    "            max_candidate_F1 = max(max_candidate_F1, summaqa_scores['avg_fscore'])\n",
    "            max_candidate_prob = max(max_candidate_prob, summaqa_scores['avg_prob'])\n",
    "            #chapter_split_sum_F1 += summaqa_scores['avg_fscore']\n",
    "            #chapter_split_sum_prob += summaqa_scores['avg_prob']\n",
    "            print(\"Candidate split #{} (Chapter split length = {}, candidate length = {}): SummaQA score = {}\".format(k, len(word_tokenize(chapter_summary_split)), len(word_tokenize(candidate_summary_split)), summaqa_scores))\n",
    "            print(candidate_summary_split)\n",
    "        print(\"Chapter split max prob = {}\".format(max_candidate_prob))\n",
    "        print(\"Chapter split max F1 = {}\".format(max_candidate_F1))\n",
    "        #print(\"Chapter split sum prob = {}\".format(chapter_split_sum_prob))\n",
    "        #print(\"Chapter split sum F1 = {}\".format(chapter_split_sum_F1))\n",
    "        chapter_summaqa_prob += max_candidate_prob\n",
    "        chapter_summaqa_F1 += max_candidate_F1\n",
    "        #chapter_summaqa_prob += chapter_split_sum_prob\n",
    "        #chapter_summaqa_F1 += chapter_split_sum_F1\n",
    "    if chapter_split_count > 0:\n",
    "        chapter_summaqa_prob /= chapter_split_count\n",
    "        chapter_summaqa_F1 /= chapter_split_count\n",
    "    else:\n",
    "        chapter_summaqa_prob = 1\n",
    "        chapter_summaqa_F1 = 1\n",
    "    average_summaqa_prob += chapter_summaqa_prob\n",
    "    average_summaqa_F1 += chapter_summaqa_F1\n",
    "    print(\"Chapter SummaQA prob = {}\".format(chapter_summaqa_prob))\n",
    "    print(\"Chapter SummaQA F1 = {}\".format(chapter_summaqa_F1))\n",
    "    \n",
    "    '''\n",
    "    # Method 2: Split by n words\n",
    "    chapter_tokenized = word_tokenize(chapter_content)\n",
    "    candidate_tokenized = word_tokenize(candidate_summary)\n",
    "    split_length = 250\n",
    "    chapter_pointer = 0\n",
    "    chapter_split_count = 0\n",
    "    chapter_summaqa_prob = 0\n",
    "    chapter_summaqa_F1 = 0\n",
    "    while chapter_pointer < len(chapter_tokenized):\n",
    "        chapter_summary_split = ' '.join(chapter_tokenized[chapter_pointer:min(len(chapter_tokenized), chapter_pointer + split_length)])\n",
    "        article = chapter_summary_split\n",
    "        masked_questions, answer_spans = question_generator.get_questions(article)\n",
    "        print(\"Chapter split #{}:\".format(chapter_split_count))\n",
    "        print(chapter_summary_split)\n",
    "        print(\"Questions:\")\n",
    "        print(masked_questions)\n",
    "        print(\"Answers:\")\n",
    "        print(answer_spans)\n",
    "        #max_candidate_score = -1\n",
    "        chapter_split_sum_F1 = 0\n",
    "        chapter_split_sum_prob = 0\n",
    "        candidate_pointer = 0\n",
    "        while candidate_pointer < len(candidate_tokenized):\n",
    "            candidate_summary_split = ' '.join(candidate_tokenized[candidate_pointer:min(len(candidate_tokenized), candidate_pointer + split_length)])\n",
    "            summaqa_scores = qa_metric.compute(masked_questions, answer_spans, candidate_summary_split)\n",
    "            #max_candidate_score = max(max_candidate_score, summaqa_scores['avg_fscore'])\n",
    "            chapter_split_sum_F1 += summaqa_scores['avg_fscore']\n",
    "            chapter_split_sum_prob += summaqa_scores['avg_prob']\n",
    "            print(\"Candidate split #{} (Chapter split length = {}, candidate length = {}): SummaQA score = {}\".format(math.floor(candidate_pointer / split_length), len(word_tokenize(chapter_summary_split)), len(word_tokenize(candidate_summary_split)), summaqa_scores))\n",
    "            print(candidate_summary_split)\n",
    "            candidate_pointer += split_length\n",
    "        print(\"Chapter split sum prob = {}\".format(chapter_split_sum_prob))\n",
    "        print(\"Chapter split sum F1 = {}\".format(chapter_split_sum_F1))\n",
    "        chapter_summaqa_prob += chapter_split_sum_prob\n",
    "        chapter_summaqa_F1 += chapter_split_sum_F1\n",
    "        #chapter_summaqa_F1 += max_candidate_score\n",
    "        chapter_pointer += split_length\n",
    "        chapter_split_count += 1\n",
    "    chapter_summaqa_prob /= chapter_split_count\n",
    "    chapter_summaqa_F1 /= chapter_split_count\n",
    "    average_summaqa_prob += chapter_summaqa_prob\n",
    "    average_summaqa_F1 += chapter_summaqa_F1\n",
    "    print(\"Chapter SummaQA prob = {}\".format(chapter_summaqa_prob))\n",
    "    print(\"Chapter SummaQA F1 = {}\".format(chapter_summaqa_F1))\n",
    "    '''\n",
    "\n",
    "average_summaqa_prob /= len(test_generated_summaries)\n",
    "average_summaqa_F1 /= len(test_generated_summaries)\n",
    "print(\"Test average SummaQA prob = {}\".format(average_summaqa_prob))\n",
    "print(\"Test average SummaQA F1 = {}\".format(average_summaqa_F1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a297f2a3-c316-4551-85d2-db5b14310f16",
   "metadata": {},
   "source": [
    "## End of [6']: Run test & evaluate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

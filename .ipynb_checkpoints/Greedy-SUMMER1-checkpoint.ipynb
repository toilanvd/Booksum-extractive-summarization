{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df16a5bf-b353-4b65-b6b6-ccf7710b25e3",
   "metadata": {},
   "source": [
    "# Greedy-BERT model for long text summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d181ac9-c187-4a40-9693-a0118eab5b53",
   "metadata": {},
   "source": [
    "## Start of [1]: Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179df10c-d582-4374-9dc7-529a4bb2af32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36db0d43-d3d1-4d87-9e43-efafa9f33bc5",
   "metadata": {},
   "source": [
    "Show training data head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126813b5-0cfb-45c0-9857-d92cbaf52ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "train = pd.read_csv(\"Dataset/train.csv\")\n",
    "val = pd.read_csv(\"Dataset/dev.csv\")\n",
    "test = pd.read_csv(\"Dataset/test.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407b4cbc-6e65-47d3-88fe-d094f5cf8483",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "train[[\"chapter_path\", \"book_id\", \"source\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac330a27-a92b-40f1-bbfe-1de78ba57ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.iloc[0]['chapter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e638c12c-2e6c-40f3-8b5e-75839ca216d3",
   "metadata": {},
   "source": [
    "## End of [1]: Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310dbb9f-a741-4c49-8b05-414a1ff8dcbc",
   "metadata": {},
   "source": [
    "## Start of [2]: Span recognition (sentence splitting in this model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9d47bb-3b15-4a11-a131-cd057934a4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad797c05-ed8f-45f3-8ea2-64fef1d01113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "sent_tokenize(\"Hello! Hello world... It's true that Mr.Ryo is here? That's true!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d16969b-79b5-4c2b-aa9d-2488207a4f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_chapters = train['chapter'].tolist()\n",
    "train_chapters = [train_chapters[i].replace(\"\\n\", \" \") for i in range(len(train_chapters))]\n",
    "val_chapters = val['chapter'].tolist()\n",
    "val_chapters = [val_chapters[i].replace(\"\\n\", \" \") for i in range(len(val_chapters))]\n",
    "test_chapters = test['chapter'].tolist()\n",
    "test_chapters = [test_chapters[i].replace(\"\\n\", \" \") for i in range(len(test_chapters))]\n",
    "\n",
    "print(train_chapters[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b145b468-a7ea-4d8a-b76e-8e5147860cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_chapters_sentences = [sent_tokenize(train_chapters[i]) for i in range(len(train_chapters))]\n",
    "val_chapters_sentences = [sent_tokenize(val_chapters[i]) for i in range(len(val_chapters))]\n",
    "test_chapters_sentences = [sent_tokenize(test_chapters[i]) for i in range(len(test_chapters))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5149c4ef-9f37-4c03-8961-019d4d3b8377",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_chapters[11])\n",
    "print(test_chapters_sentences[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9083aeb-54c5-4ccf-96e1-bfdfd6c4f1f3",
   "metadata": {},
   "source": [
    "## End of [2]: Span recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bb0b93-563f-4abc-827d-cccc663cf5b9",
   "metadata": {},
   "source": [
    "## Start of [3]: (BERT + cosine) score function for sentence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b462624b-194e-4b1d-b98a-18e07b93c53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "sentence_embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2', device = device)  # multi-language model  \n",
    "\n",
    "def sentence_similarity(sentence_A, sentence_B):\n",
    "    temp_sentence_A = [sentence_A]\n",
    "    temp_embedding_A = sentence_embedding_model.encode(temp_sentence_A, convert_to_tensor=False)\n",
    "    temp_sentence_B = [sentence_B]\n",
    "    temp_embedding_B = sentence_embedding_model.encode(temp_sentence_B, convert_to_tensor=False)\n",
    "    cosine_score = util.cos_sim(temp_embedding_A, temp_embedding_B)\n",
    "    return cosine_score[0][0].item()\n",
    "\n",
    "print(train_chapters_sentences[0][2])\n",
    "print(train_chapters_sentences[0][3])\n",
    "print(sentence_similarity(train_chapters_sentences[0][2], train_chapters_sentences[0][3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09c052a-85b1-4f60-b13c-16b8bc851a34",
   "metadata": {},
   "source": [
    "## End of [3]: (BERT + cosine) score function for sentence similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50387011-e37e-4d2a-afc2-3fc0bcdf876c",
   "metadata": {},
   "source": [
    "## Start of [5']: Summary function & validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd7a6a8-9021-4e9c-a641-e832b2566c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average summary length vs chapter length ratio\n",
    "train_chapters_length = train['chapter_length'].tolist()\n",
    "train_summaries_length = train['summary_length'].tolist()\n",
    "val_chapters_length = val['chapter_length'].tolist()\n",
    "test_chapters_length = test['chapter_length'].tolist()\n",
    "average_summary_length_ratio = 0\n",
    "for i in range(len(train_chapters_length)):\n",
    "    average_summary_length_ratio += train_summaries_length[i] / train_chapters_length[i]\n",
    "    #print(\"{} - {}\".format(i, train_summaries_length[i] / train_chapters_length[i]))\n",
    "average_summary_length_ratio /= len(train_chapters_length)\n",
    "print(average_summary_length_ratio)\n",
    "print(' '.join(\"    lsdfl    sdjfkjs  \".split(\" \")).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81759ca9-43f8-4f76-a4b6-e9eb0a673c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
    "nsp_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # nsp: next sentence prediction\n",
    "nsp_model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def next_sentence_score(sentence_A, sentence_B):\n",
    "    nsp_tokenized = nsp_tokenizer(sentence_A, sentence_B, return_tensors='pt')\n",
    "    nsp_labels = torch.LongTensor([0])\n",
    "    nsp_scores = nsp_model(**nsp_tokenized, labels = nsp_labels)\n",
    "    if nsp_scores[1][0][0].item() > nsp_scores[1][0][1].item():\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d55626-9931-4f77-8443-8624e7a79fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train summary generation\n",
    "train_generated_summaries = []\n",
    "for i in range(len(train_chapters)):\n",
    "    print(\"Train doc {}: {} sentences\".format(i, len(train_chapters_sentences[i])))\n",
    "    summary_length_limit = math.ceil(train_chapters_length[i] * average_summary_length_ratio)\n",
    "\n",
    "    pairwise_sentence_scores = np.zeros((len(train_chapters_sentences[i]), len(train_chapters_sentences[i])))\n",
    "    for j in range(len(train_chapters_sentences[i])):\n",
    "        for k in range(len(train_chapters_sentences[i])):\n",
    "            pairwise_sentence_scores[j, k] = next_sentence_score(train_chapters_sentences[i][j], train_chapters_sentences[i][k])\n",
    "    \n",
    "    sentence_sum_similarity = [(0, j) for j in range(len(train_chapters_sentences[i]))]\n",
    "    for j in range(len(train_chapters_sentences[i])):\n",
    "        for k in range(len(train_chapters_sentences[i])):\n",
    "            #print(\"j = {} - k = {}\".format(j, k))\n",
    "            sentence_sum_similarity[j] = (sentence_sum_similarity[j][0] + pairwise_sentence_scores[j][k], j)\n",
    "    sentence_sum_similarity.sort(reverse = True)\n",
    "    generated_summary_length = 0\n",
    "    chosen_sentences_index = []\n",
    "    for j in range(len(train_chapters_sentences[i])):\n",
    "        chosen_sentences_index.append(sentence_sum_similarity[j][1])\n",
    "        generated_summary_length += len(' '.join(train_chapters_sentences[i][sentence_sum_similarity[j][1]].split(\" \")).split())\n",
    "        if generated_summary_length >= summary_length_limit:\n",
    "            break\n",
    "    chosen_sentences_index.sort()\n",
    "    generated_summary = \"\"\n",
    "    for j in range(len(chosen_sentences_index)):\n",
    "        if j > 0:\n",
    "            generated_summary += \" \"\n",
    "        generated_summary += train_chapters_sentences[i][chosen_sentences_index[j]]\n",
    "    train_generated_summaries.append(generated_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51114b7e-73a6-4adf-8df8-88ccf387dcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_generated_summaries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668266b7-3273-4a67-aa99-7173dc8c1c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train reference summaries\n",
    "train_reference_summaries = train['summary_text'].tolist()\n",
    "train_reference_summaries = [train_reference_summaries[i].replace(\"\\n\", \" \") for i in range(len(train_reference_summaries))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a413c054-9277-4569-a2ec-a6e6b7728660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "rougescorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2175a8-9eff-4c51-a25d-03af80e8cb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ROUGE score\n",
    "train_rouge1 = 0\n",
    "train_rouge2 = 0\n",
    "train_rougeL = 0\n",
    "for i in range(len(train_generated_summaries)):\n",
    "    print(i)\n",
    "    scores = rougescorer.score(train_reference_summaries[i], train_generated_summaries[i])\n",
    "    for key in scores:\n",
    "        print(\"{}: {}\".format(key, scores[key]))\n",
    "        if key == \"rouge1\":\n",
    "            train_rouge1 += scores[key][2] # take fmeasure value\n",
    "        elif key == \"rouge2\":\n",
    "            train_rouge2 += scores[key][2] # take fmeasure value\n",
    "        else:\n",
    "            train_rougeL += scores[key][2] # take fmeasure value\n",
    "train_rouge1 /= len(train_generated_summaries)\n",
    "train_rouge2 /= len(train_generated_summaries)\n",
    "train_rougeL /= len(train_generated_summaries)\n",
    "print(\"Train: rouge1 = {}, rouge2 = {}, rougeL = {}\".format(train_rouge1, train_rouge2, train_rougeL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f8383d-de4e-4e83-8bde-630c17e6cf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert_score\n",
    "from bert_score import BERTScorer\n",
    "\n",
    "bertscorer = BERTScorer(lang='en', rescale_with_baseline=True) # Default using 'roberta-large' model\n",
    "#bertscorer = BERTScorer(model_type='xlnet-large-cased', rescale_with_baseline=True, lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141808b9-00f1-46db-9c24-a11e744e715a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train BERT score\n",
    "average_bertscore_F1 = 0\n",
    "\n",
    "for i in range(len(train_generated_summaries)):\n",
    "    print(\"Train doc {}:\".format(i))\n",
    "    \n",
    "    ref_summary = copy.deepcopy(train_reference_summaries[i])\n",
    "    candidate_summary = copy.deepcopy(train_generated_summaries[i])\n",
    "\n",
    "    # Method 1: Split by sentence\n",
    "    ref_sentences = sent_tokenize(ref_summary)\n",
    "    candidate_sentences = sent_tokenize(candidate_summary)\n",
    "    chapter_bertscore_F1 = 0\n",
    "    for j in range(len(ref_sentences)):\n",
    "        ref_summary_split = ref_sentences[j]\n",
    "        max_candidate_score = -1\n",
    "        for k in range(len(candidate_sentences)):\n",
    "            candidate_summary_split = candidate_sentences[k]\n",
    "            P, R, F1 = bertscorer.score([candidate_summary_split], [ref_summary_split])\n",
    "            #print(\"Candidate length = {}, reference length = {}\".format(len(word_tokenize(candidate_summary_split)), len(word_tokenize(ref_summary_split))))\n",
    "            max_candidate_score = max(max_candidate_score, F1.mean().item())\n",
    "        chapter_bertscore_F1 += max_candidate_score\n",
    "    chapter_bertscore_F1 /= len(ref_sentences)\n",
    "    average_bertscore_F1 += chapter_bertscore_F1\n",
    "    print(\"Chapter BERTScore F1 = {}\".format(chapter_bertscore_F1))\n",
    "\n",
    "    '''\n",
    "    # Method 2: Split by n words\n",
    "    ref_tokenized = word_tokenize(ref_summary)\n",
    "    candidate_tokenized = word_tokenize(candidate_summary)\n",
    "    split_length = 250\n",
    "    ref_pointer = 0\n",
    "    ref_split_count = 0\n",
    "    chapter_bertscore_F1 = 0\n",
    "    while ref_pointer < len(ref_tokenized):\n",
    "        ref_summary_split = ' '.join(ref_tokenized[ref_pointer:min(len(ref_tokenized), ref_pointer + split_length)])\n",
    "        max_candidate_score = -1\n",
    "        candidate_pointer = 0\n",
    "        while candidate_pointer < len(candidate_tokenized):\n",
    "            candidate_summary_split = ' '.join(candidate_tokenized[candidate_pointer:min(len(candidate_tokenized), candidate_pointer + split_length)])\n",
    "            P, R, F1 = bertscorer.score([candidate_summary_split], [ref_summary_split])\n",
    "            #print(\"Candidate length = {}, reference length = {}\".format(len(word_tokenize(candidate_summary_split)), len(word_tokenize(ref_summary_split))))\n",
    "            max_candidate_score = max(max_candidate_score, F1.mean().item())\n",
    "            candidate_pointer += split_length\n",
    "        chapter_bertscore_F1 += max_candidate_score\n",
    "        ref_pointer += split_length\n",
    "        ref_split_count += 1\n",
    "    chapter_bertscore_F1 /= ref_split_count\n",
    "    average_bertscore_F1 += chapter_bertscore_F1\n",
    "    print(\"Chapter BERTScore F1 = {}\".format(chapter_bertscore_F1))\n",
    "    '''\n",
    "average_bertscore_F1 /= len(train_generated_summaries)\n",
    "print(\"Train average BERTscore F1 = {}\".format(average_bertscore_F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8928a855-f56d-4b51-ad69-b9fc422f7c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from summaqa import evaluate_corpus\n",
    "from summaqa import QG_masked\n",
    "question_generator = QG_masked()\n",
    "from summaqa import QA_Metric\n",
    "qa_metric = QA_Metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02800aea-7a01-49b1-93d8-71663f480608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train summaQA\n",
    "#srcs = train_chapters[:len(train_generated_summaries)]\n",
    "#gens = train_generated_summaries\n",
    "#srcs = [' '.join(' '.join(srcs[i].split(\" \")).split()[:300]) for i in range(len(srcs))]\n",
    "#gens = [' '.join(' '.join(gens[i].split(\" \")).split()[:300]) for i in range(len(gens))]\n",
    "#evaluate_corpus(srcs, gens)\n",
    "\n",
    "average_summaqa_prob = 0\n",
    "average_summaqa_F1 = 0\n",
    "\n",
    "for i in range(len(train_generated_summaries)):\n",
    "    print(\"Train doc {}:\".format(i))\n",
    "    \n",
    "    chapter_content = copy.deepcopy(train_chapters[i])\n",
    "    candidate_summary = copy.deepcopy(train_generated_summaries[i])\n",
    "\n",
    "    # Method 1: Split by sentence\n",
    "    chapter_sentences = train_chapters_sentences[i]\n",
    "    candidate_sentences = sent_tokenize(candidate_summary)\n",
    "    chapter_split_count = 0\n",
    "    chapter_summaqa_prob = 0\n",
    "    chapter_summaqa_F1 = 0\n",
    "    for j in range(len(chapter_sentences)):\n",
    "        chapter_summary_split = chapter_sentences[j]\n",
    "        article = chapter_summary_split\n",
    "        masked_questions, answer_spans = question_generator.get_questions(article)\n",
    "        if len(masked_questions) == 0:\n",
    "            continue\n",
    "        chapter_split_count += 1\n",
    "        print(\"Chapter split #{}:\".format(j))\n",
    "        print(chapter_summary_split)\n",
    "        print(\"Questions:\")\n",
    "        print(masked_questions)\n",
    "        print(\"Answers:\")\n",
    "        print(answer_spans)\n",
    "        max_candidate_F1 = 0\n",
    "        max_candidate_prob = 0\n",
    "        #chapter_split_sum_F1 = 0\n",
    "        #chapter_split_sum_prob = 0\n",
    "        for k in range(len(candidate_sentences)):\n",
    "            candidate_summary_split = candidate_sentences[k]\n",
    "            summaqa_scores = qa_metric.compute(masked_questions, answer_spans, candidate_summary_split)\n",
    "            max_candidate_F1 = max(max_candidate_F1, summaqa_scores['avg_fscore'])\n",
    "            max_candidate_prob = max(max_candidate_prob, summaqa_scores['avg_prob'])\n",
    "            #chapter_split_sum_F1 += summaqa_scores['avg_fscore']\n",
    "            #chapter_split_sum_prob += summaqa_scores['avg_prob']\n",
    "            print(\"Candidate split #{} (Chapter split length = {}, candidate length = {}): SummaQA score = {}\".format(k, len(word_tokenize(chapter_summary_split)), len(word_tokenize(candidate_summary_split)), summaqa_scores))\n",
    "            print(candidate_summary_split)\n",
    "        print(\"Chapter split max prob = {}\".format(max_candidate_prob))\n",
    "        print(\"Chapter split max F1 = {}\".format(max_candidate_F1))\n",
    "        #print(\"Chapter split sum prob = {}\".format(chapter_split_sum_prob))\n",
    "        #print(\"Chapter split sum F1 = {}\".format(chapter_split_sum_F1))\n",
    "        chapter_summaqa_prob += max_candidate_prob\n",
    "        chapter_summaqa_F1 += max_candidate_F1\n",
    "        #chapter_summaqa_prob += chapter_split_sum_prob\n",
    "        #chapter_summaqa_F1 += chapter_split_sum_F1\n",
    "    if chapter_split_count > 0:\n",
    "        chapter_summaqa_prob /= chapter_split_count\n",
    "        chapter_summaqa_F1 /= chapter_split_count\n",
    "    else:\n",
    "        chapter_summaqa_prob = 1\n",
    "        chapter_summaqa_F1 = 1\n",
    "    average_summaqa_prob += chapter_summaqa_prob\n",
    "    average_summaqa_F1 += chapter_summaqa_F1\n",
    "    print(\"Chapter SummaQA prob = {}\".format(chapter_summaqa_prob))\n",
    "    print(\"Chapter SummaQA F1 = {}\".format(chapter_summaqa_F1))\n",
    "    \n",
    "    '''\n",
    "    # Method 2: Split by n words\n",
    "    chapter_tokenized = word_tokenize(chapter_content)\n",
    "    candidate_tokenized = word_tokenize(candidate_summary)\n",
    "    split_length = 250\n",
    "    chapter_pointer = 0\n",
    "    chapter_split_count = 0\n",
    "    chapter_summaqa_prob = 0\n",
    "    chapter_summaqa_F1 = 0\n",
    "    while chapter_pointer < len(chapter_tokenized):\n",
    "        chapter_summary_split = ' '.join(chapter_tokenized[chapter_pointer:min(len(chapter_tokenized), chapter_pointer + split_length)])\n",
    "        article = chapter_summary_split\n",
    "        masked_questions, answer_spans = question_generator.get_questions(article)\n",
    "        print(\"Chapter split #{}:\".format(chapter_split_count))\n",
    "        print(chapter_summary_split)\n",
    "        print(\"Questions:\")\n",
    "        print(masked_questions)\n",
    "        print(\"Answers:\")\n",
    "        print(answer_spans)\n",
    "        #max_candidate_score = -1\n",
    "        chapter_split_sum_F1 = 0\n",
    "        chapter_split_sum_prob = 0\n",
    "        candidate_pointer = 0\n",
    "        while candidate_pointer < len(candidate_tokenized):\n",
    "            candidate_summary_split = ' '.join(candidate_tokenized[candidate_pointer:min(len(candidate_tokenized), candidate_pointer + split_length)])\n",
    "            summaqa_scores = qa_metric.compute(masked_questions, answer_spans, candidate_summary_split)\n",
    "            #max_candidate_score = max(max_candidate_score, summaqa_scores['avg_fscore'])\n",
    "            chapter_split_sum_F1 += summaqa_scores['avg_fscore']\n",
    "            chapter_split_sum_prob += summaqa_scores['avg_prob']\n",
    "            print(\"Candidate split #{} (Chapter split length = {}, candidate length = {}): SummaQA score = {}\".format(math.floor(candidate_pointer / split_length), len(word_tokenize(chapter_summary_split)), len(word_tokenize(candidate_summary_split)), summaqa_scores))\n",
    "            print(candidate_summary_split)\n",
    "            candidate_pointer += split_length\n",
    "        print(\"Chapter split sum prob = {}\".format(chapter_split_sum_prob))\n",
    "        print(\"Chapter split sum F1 = {}\".format(chapter_split_sum_F1))\n",
    "        chapter_summaqa_prob += chapter_split_sum_prob\n",
    "        chapter_summaqa_F1 += chapter_split_sum_F1\n",
    "        #chapter_summaqa_F1 += max_candidate_score\n",
    "        chapter_pointer += split_length\n",
    "        chapter_split_count += 1\n",
    "    chapter_summaqa_prob /= chapter_split_count\n",
    "    chapter_summaqa_F1 /= chapter_split_count\n",
    "    average_summaqa_prob += chapter_summaqa_prob\n",
    "    average_summaqa_F1 += chapter_summaqa_F1\n",
    "    print(\"Chapter SummaQA prob = {}\".format(chapter_summaqa_prob))\n",
    "    print(\"Chapter SummaQA F1 = {}\".format(chapter_summaqa_F1))\n",
    "    '''\n",
    "\n",
    "average_summaqa_prob /= len(train_generated_summaries)\n",
    "average_summaqa_F1 /= len(train_generated_summaries)\n",
    "print(\"Train average SummaQA prob = {}\".format(average_summaqa_prob))\n",
    "print(\"Train average SummaQA F1 = {}\".format(average_summaqa_F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de075eb-3f89-4430-8130-79b94158dd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation summary generation\n",
    "val_generated_summaries = []\n",
    "for i in range(len(val_chapters)):\n",
    "    print(\"Validation doc {}: {} sentences\".format(i, len(val_chapters_sentences[i])))\n",
    "    summary_length_limit = math.ceil(val_chapters_length[i] * average_summary_length_ratio)\n",
    "\n",
    "    pairwise_sentence_scores = np.zeros((len(val_chapters_sentences[i]), len(val_chapters_sentences[i])))\n",
    "    for j in range(len(val_chapters_sentences[i])):\n",
    "        for k in range(len(val_chapters_sentences[i])):\n",
    "            pairwise_sentence_scores[j, k] = next_sentence_score(val_chapters_sentences[i][j], val_chapters_sentences[i][k])\n",
    "    \n",
    "    sentence_sum_similarity = [(0, j) for j in range(len(val_chapters_sentences[i]))]\n",
    "    for j in range(len(val_chapters_sentences[i])):\n",
    "        for k in range(len(val_chapters_sentences[i])):\n",
    "            #print(\"j = {} - k = {}\".format(j, k))\n",
    "            sentence_sum_similarity[j] = (sentence_sum_similarity[j][0] + pairwise_sentence_scores[j][k], j)\n",
    "    sentence_sum_similarity.sort(reverse = True)\n",
    "    generated_summary_length = 0\n",
    "    chosen_sentences_index = []\n",
    "    for j in range(len(val_chapters_sentences[i])):\n",
    "        chosen_sentences_index.append(sentence_sum_similarity[j][1])\n",
    "        generated_summary_length += len(' '.join(val_chapters_sentences[i][sentence_sum_similarity[j][1]].split(\" \")).split())\n",
    "        if generated_summary_length >= summary_length_limit:\n",
    "            break\n",
    "    chosen_sentences_index.sort()\n",
    "    generated_summary = \"\"\n",
    "    for j in range(len(chosen_sentences_index)):\n",
    "        if j > 0:\n",
    "            generated_summary += \" \"\n",
    "        generated_summary += val_chapters_sentences[i][chosen_sentences_index[j]]\n",
    "    val_generated_summaries.append(generated_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc37c500-4242-4b80-b3a0-6ad33e7c5353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation reference summaries\n",
    "val_reference_summaries = val['summary_text'].tolist()\n",
    "val_reference_summaries = [val_reference_summaries[i].replace(\"\\n\", \" \") for i in range(len(val_reference_summaries))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a80f92-7595-4540-bb79-964d1c2f1655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation ROUGE score\n",
    "val_rouge1 = 0\n",
    "val_rouge2 = 0\n",
    "val_rougeL = 0\n",
    "for i in range(len(val_generated_summaries)):\n",
    "    print(i)\n",
    "    scores = rougescorer.score(val_reference_summaries[i], val_generated_summaries[i])\n",
    "    for key in scores:\n",
    "        print(\"{}: {}\".format(key, scores[key]))\n",
    "        if key == \"rouge1\":\n",
    "            val_rouge1 += scores[key][2] # take fmeasure value\n",
    "        elif key == \"rouge2\":\n",
    "            val_rouge2 += scores[key][2] # take fmeasure value\n",
    "        else:\n",
    "            val_rougeL += scores[key][2] # take fmeasure value\n",
    "val_rouge1 /= len(val_generated_summaries)\n",
    "val_rouge2 /= len(val_generated_summaries)\n",
    "val_rougeL /= len(val_generated_summaries)\n",
    "print(\"Validation: rouge1 = {}, rouge2 = {}, rougeL = {}\".format(val_rouge1, val_rouge2, val_rougeL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f221430b-f8d9-4e55-84bd-715767ebd73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation BERT score\n",
    "average_bertscore_F1 = 0\n",
    "\n",
    "for i in range(len(val_generated_summaries)):\n",
    "    print(\"Validation doc {}:\".format(i))\n",
    "    \n",
    "    ref_summary = copy.deepcopy(val_reference_summaries[i])\n",
    "    candidate_summary = copy.deepcopy(val_generated_summaries[i])\n",
    "\n",
    "    # Method 1: Split by sentence\n",
    "    ref_sentences = sent_tokenize(ref_summary)\n",
    "    candidate_sentences = sent_tokenize(candidate_summary)\n",
    "    chapter_bertscore_F1 = 0\n",
    "    for j in range(len(ref_sentences)):\n",
    "        ref_summary_split = ref_sentences[j]\n",
    "        max_candidate_score = -1\n",
    "        for k in range(len(candidate_sentences)):\n",
    "            candidate_summary_split = candidate_sentences[k]\n",
    "            P, R, F1 = bertscorer.score([candidate_summary_split], [ref_summary_split])\n",
    "            #print(\"Candidate length = {}, reference length = {}\".format(len(word_tokenize(candidate_summary_split)), len(word_tokenize(ref_summary_split))))\n",
    "            max_candidate_score = max(max_candidate_score, F1.mean().item())\n",
    "        chapter_bertscore_F1 += max_candidate_score\n",
    "    chapter_bertscore_F1 /= len(ref_sentences)\n",
    "    average_bertscore_F1 += chapter_bertscore_F1\n",
    "    print(\"Chapter BERTScore F1 = {}\".format(chapter_bertscore_F1))\n",
    "\n",
    "    '''\n",
    "    # Method 2: Split by n words\n",
    "    ref_tokenized = word_tokenize(ref_summary)\n",
    "    candidate_tokenized = word_tokenize(candidate_summary)\n",
    "    split_length = 250\n",
    "    ref_pointer = 0\n",
    "    ref_split_count = 0\n",
    "    chapter_bertscore_F1 = 0\n",
    "    while ref_pointer < len(ref_tokenized):\n",
    "        ref_summary_split = ' '.join(ref_tokenized[ref_pointer:min(len(ref_tokenized), ref_pointer + split_length)])\n",
    "        max_candidate_score = -1\n",
    "        candidate_pointer = 0\n",
    "        while candidate_pointer < len(candidate_tokenized):\n",
    "            candidate_summary_split = ' '.join(candidate_tokenized[candidate_pointer:min(len(candidate_tokenized), candidate_pointer + split_length)])\n",
    "            P, R, F1 = bertscorer.score([candidate_summary_split], [ref_summary_split])\n",
    "            #print(\"Candidate length = {}, reference length = {}\".format(len(word_tokenize(candidate_summary_split)), len(word_tokenize(ref_summary_split))))\n",
    "            max_candidate_score = max(max_candidate_score, F1.mean().item())\n",
    "            candidate_pointer += split_length\n",
    "        chapter_bertscore_F1 += max_candidate_score\n",
    "        ref_pointer += split_length\n",
    "        ref_split_count += 1\n",
    "    chapter_bertscore_F1 /= ref_split_count\n",
    "    average_bertscore_F1 += chapter_bertscore_F1\n",
    "    print(\"Chapter BERTScore F1 = {}\".format(chapter_bertscore_F1))\n",
    "    '''\n",
    "average_bertscore_F1 /= len(val_generated_summaries)\n",
    "print(\"Validation average BERTscore F1 = {}\".format(average_bertscore_F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41aab64-8ee0-485e-8c35-26b78c9ee6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation summaQA\n",
    "#srcs = val_chapters[:len(val_generated_summaries)]\n",
    "#gens = val_generated_summaries\n",
    "#srcs = [' '.join(' '.join(srcs[i].split(\" \")).split()[:300]) for i in range(len(srcs))]\n",
    "#gens = [' '.join(' '.join(gens[i].split(\" \")).split()[:300]) for i in range(len(gens))]\n",
    "#evaluate_corpus(srcs, gens)\n",
    "\n",
    "average_summaqa_prob = 0\n",
    "average_summaqa_F1 = 0\n",
    "\n",
    "for i in range(len(val_generated_summaries)):\n",
    "    print(\"Validation doc {}:\".format(i))\n",
    "    \n",
    "    chapter_content = copy.deepcopy(val_chapters[i])\n",
    "    candidate_summary = copy.deepcopy(val_generated_summaries[i])\n",
    "\n",
    "    # Method 1: Split by sentence\n",
    "    chapter_sentences = val_chapters_sentences[i]\n",
    "    candidate_sentences = sent_tokenize(candidate_summary)\n",
    "    chapter_split_count = 0\n",
    "    chapter_summaqa_prob = 0\n",
    "    chapter_summaqa_F1 = 0\n",
    "    for j in range(len(chapter_sentences)):\n",
    "        chapter_summary_split = chapter_sentences[j]\n",
    "        article = chapter_summary_split\n",
    "        masked_questions, answer_spans = question_generator.get_questions(article)\n",
    "        if len(masked_questions) == 0:\n",
    "            continue\n",
    "        chapter_split_count += 1\n",
    "        print(\"Chapter split #{}:\".format(j))\n",
    "        print(chapter_summary_split)\n",
    "        print(\"Questions:\")\n",
    "        print(masked_questions)\n",
    "        print(\"Answers:\")\n",
    "        print(answer_spans)\n",
    "        max_candidate_F1 = 0\n",
    "        max_candidate_prob = 0\n",
    "        #chapter_split_sum_F1 = 0\n",
    "        #chapter_split_sum_prob = 0\n",
    "        for k in range(len(candidate_sentences)):\n",
    "            candidate_summary_split = candidate_sentences[k]\n",
    "            summaqa_scores = qa_metric.compute(masked_questions, answer_spans, candidate_summary_split)\n",
    "            max_candidate_F1 = max(max_candidate_F1, summaqa_scores['avg_fscore'])\n",
    "            max_candidate_prob = max(max_candidate_prob, summaqa_scores['avg_prob'])\n",
    "            #chapter_split_sum_F1 += summaqa_scores['avg_fscore']\n",
    "            #chapter_split_sum_prob += summaqa_scores['avg_prob']\n",
    "            print(\"Candidate split #{} (Chapter split length = {}, candidate length = {}): SummaQA score = {}\".format(k, len(word_tokenize(chapter_summary_split)), len(word_tokenize(candidate_summary_split)), summaqa_scores))\n",
    "            print(candidate_summary_split)\n",
    "        print(\"Chapter split max prob = {}\".format(max_candidate_prob))\n",
    "        print(\"Chapter split max F1 = {}\".format(max_candidate_F1))\n",
    "        #print(\"Chapter split sum prob = {}\".format(chapter_split_sum_prob))\n",
    "        #print(\"Chapter split sum F1 = {}\".format(chapter_split_sum_F1))\n",
    "        chapter_summaqa_prob += max_candidate_prob\n",
    "        chapter_summaqa_F1 += max_candidate_F1\n",
    "        #chapter_summaqa_prob += chapter_split_sum_prob\n",
    "        #chapter_summaqa_F1 += chapter_split_sum_F1\n",
    "    if chapter_split_count > 0:\n",
    "        chapter_summaqa_prob /= chapter_split_count\n",
    "        chapter_summaqa_F1 /= chapter_split_count\n",
    "    else:\n",
    "        chapter_summaqa_prob = 1\n",
    "        chapter_summaqa_F1 = 1\n",
    "    average_summaqa_prob += chapter_summaqa_prob\n",
    "    average_summaqa_F1 += chapter_summaqa_F1\n",
    "    print(\"Chapter SummaQA prob = {}\".format(chapter_summaqa_prob))\n",
    "    print(\"Chapter SummaQA F1 = {}\".format(chapter_summaqa_F1))\n",
    "    \n",
    "    '''\n",
    "    # Method 2: Split by n words\n",
    "    chapter_tokenized = word_tokenize(chapter_content)\n",
    "    candidate_tokenized = word_tokenize(candidate_summary)\n",
    "    split_length = 250\n",
    "    chapter_pointer = 0\n",
    "    chapter_split_count = 0\n",
    "    chapter_summaqa_prob = 0\n",
    "    chapter_summaqa_F1 = 0\n",
    "    while chapter_pointer < len(chapter_tokenized):\n",
    "        chapter_summary_split = ' '.join(chapter_tokenized[chapter_pointer:min(len(chapter_tokenized), chapter_pointer + split_length)])\n",
    "        article = chapter_summary_split\n",
    "        masked_questions, answer_spans = question_generator.get_questions(article)\n",
    "        print(\"Chapter split #{}:\".format(chapter_split_count))\n",
    "        print(chapter_summary_split)\n",
    "        print(\"Questions:\")\n",
    "        print(masked_questions)\n",
    "        print(\"Answers:\")\n",
    "        print(answer_spans)\n",
    "        #max_candidate_score = -1\n",
    "        chapter_split_sum_F1 = 0\n",
    "        chapter_split_sum_prob = 0\n",
    "        candidate_pointer = 0\n",
    "        while candidate_pointer < len(candidate_tokenized):\n",
    "            candidate_summary_split = ' '.join(candidate_tokenized[candidate_pointer:min(len(candidate_tokenized), candidate_pointer + split_length)])\n",
    "            summaqa_scores = qa_metric.compute(masked_questions, answer_spans, candidate_summary_split)\n",
    "            #max_candidate_score = max(max_candidate_score, summaqa_scores['avg_fscore'])\n",
    "            chapter_split_sum_F1 += summaqa_scores['avg_fscore']\n",
    "            chapter_split_sum_prob += summaqa_scores['avg_prob']\n",
    "            print(\"Candidate split #{} (Chapter split length = {}, candidate length = {}): SummaQA score = {}\".format(math.floor(candidate_pointer / split_length), len(word_tokenize(chapter_summary_split)), len(word_tokenize(candidate_summary_split)), summaqa_scores))\n",
    "            print(candidate_summary_split)\n",
    "            candidate_pointer += split_length\n",
    "        print(\"Chapter split sum prob = {}\".format(chapter_split_sum_prob))\n",
    "        print(\"Chapter split sum F1 = {}\".format(chapter_split_sum_F1))\n",
    "        chapter_summaqa_prob += chapter_split_sum_prob\n",
    "        chapter_summaqa_F1 += chapter_split_sum_F1\n",
    "        #chapter_summaqa_F1 += max_candidate_score\n",
    "        chapter_pointer += split_length\n",
    "        chapter_split_count += 1\n",
    "    chapter_summaqa_prob /= chapter_split_count\n",
    "    chapter_summaqa_F1 /= chapter_split_count\n",
    "    average_summaqa_prob += chapter_summaqa_prob\n",
    "    average_summaqa_F1 += chapter_summaqa_F1\n",
    "    print(\"Chapter SummaQA prob = {}\".format(chapter_summaqa_prob))\n",
    "    print(\"Chapter SummaQA F1 = {}\".format(chapter_summaqa_F1))\n",
    "    '''\n",
    "\n",
    "average_summaqa_prob /= len(val_generated_summaries)\n",
    "average_summaqa_F1 /= len(val_generated_summaries)\n",
    "print(\"Validation average SummaQA prob = {}\".format(average_summaqa_prob))\n",
    "print(\"Validation average SummaQA F1 = {}\".format(average_summaqa_F1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d104c54-16c0-4c8f-81b8-a9d064bc0f67",
   "metadata": {},
   "source": [
    "## End of [5']: Summary function & validation score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dde843-5d32-4e0e-a37e-4c1e216b8429",
   "metadata": {},
   "source": [
    "## Start of [6']: Run test & evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d999411-d0c4-4c56-ab13-71f4158d50cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test doc 0: 77 sentences\n",
      "Test doc 1: 146 sentences\n",
      "Test doc 2: 67 sentences\n",
      "Test doc 3: 12 sentences\n",
      "Test doc 4: 21 sentences\n",
      "Test doc 5: 123 sentences\n",
      "Test doc 6: 7 sentences\n",
      "Test doc 7: 18 sentences\n",
      "Test doc 8: 31 sentences\n",
      "Test doc 9: 63 sentences\n",
      "Test doc 10: 202 sentences\n",
      "Test doc 11: 4 sentences\n",
      "Test doc 12: 5 sentences\n",
      "Test doc 13: 68 sentences\n",
      "Test doc 14: 21 sentences\n",
      "Test doc 15: 12 sentences\n",
      "Test doc 16: 34 sentences\n",
      "Test doc 17: 79 sentences\n",
      "Test doc 18: 38 sentences\n",
      "Test doc 19: 29 sentences\n",
      "Test doc 20: 12 sentences\n",
      "Test doc 21: 21 sentences\n",
      "Test doc 22: 69 sentences\n",
      "Test doc 23: 33 sentences\n",
      "Test doc 24: 21 sentences\n",
      "Test doc 25: 7 sentences\n",
      "Test doc 26: 18 sentences\n",
      "Test doc 27: 31 sentences\n",
      "Test doc 28: 40 sentences\n",
      "Test doc 29: 63 sentences\n",
      "Test doc 30: 44 sentences\n",
      "Test doc 31: 50 sentences\n",
      "Test doc 32: 9 sentences\n",
      "Test doc 33: 20 sentences\n",
      "Test doc 34: 9 sentences\n",
      "Test doc 35: 28 sentences\n",
      "Test doc 36: 42 sentences\n",
      "Test doc 37: 9 sentences\n",
      "Test doc 38: 68 sentences\n",
      "Test doc 39: 33 sentences\n",
      "Test doc 40: 34 sentences\n",
      "Test doc 41: 79 sentences\n",
      "Test doc 42: 38 sentences\n",
      "Test doc 43: 29 sentences\n",
      "Test doc 44: 12 sentences\n",
      "Test doc 45: 21 sentences\n",
      "Test doc 46: 69 sentences\n",
      "Test doc 47: 33 sentences\n",
      "Test doc 48: 21 sentences\n",
      "Test doc 49: 7 sentences\n",
      "Test doc 50: 18 sentences\n",
      "Test doc 51: 31 sentences\n",
      "Test doc 52: 40 sentences\n",
      "Test doc 53: 63 sentences\n",
      "Test doc 54: 44 sentences\n",
      "Test doc 55: 50 sentences\n",
      "Test doc 56: 29 sentences\n",
      "Test doc 57: 9 sentences\n",
      "Test doc 58: 28 sentences\n",
      "Test doc 59: 42 sentences\n",
      "Test doc 60: 4 sentences\n",
      "Test doc 61: 5 sentences\n",
      "Test doc 62: 68 sentences\n",
      "Test doc 63: 21 sentences\n",
      "Test doc 64: 12 sentences\n",
      "Test doc 65: 34 sentences\n",
      "Test doc 66: 79 sentences\n",
      "Test doc 67: 38 sentences\n",
      "Test doc 68: 29 sentences\n",
      "Test doc 69: 12 sentences\n",
      "Test doc 70: 21 sentences\n",
      "Test doc 71: 69 sentences\n",
      "Test doc 72: 33 sentences\n",
      "Test doc 73: 21 sentences\n",
      "Test doc 74: 7 sentences\n",
      "Test doc 75: 18 sentences\n",
      "Test doc 76: 31 sentences\n",
      "Test doc 77: 40 sentences\n",
      "Test doc 78: 63 sentences\n",
      "Test doc 79: 44 sentences\n",
      "Test doc 80: 50 sentences\n",
      "Test doc 81: 9 sentences\n",
      "Test doc 82: 20 sentences\n",
      "Test doc 83: 9 sentences\n",
      "Test doc 84: 28 sentences\n",
      "Test doc 85: 42 sentences\n",
      "Test doc 86: 50 sentences\n",
      "Test doc 87: 99 sentences\n",
      "Test doc 88: 80 sentences\n",
      "Test doc 89: 88 sentences\n",
      "Test doc 90: 33 sentences\n",
      "Test doc 91: 44 sentences\n",
      "Test doc 92: 41 sentences\n",
      "Test doc 93: 62 sentences\n",
      "Test doc 94: 80 sentences\n",
      "Test doc 95: 84 sentences\n",
      "Test doc 96: 58 sentences\n",
      "Test doc 97: 67 sentences\n",
      "Test doc 98: 133 sentences\n",
      "Test doc 99: 70 sentences\n",
      "Test doc 100: 139 sentences\n",
      "Test doc 101: 104 sentences\n",
      "Test doc 102: 114 sentences\n",
      "Test doc 103: 76 sentences\n",
      "Test doc 104: 140 sentences\n",
      "Test doc 105: 156 sentences\n",
      "Test doc 106: 107 sentences\n",
      "Test doc 107: 125 sentences\n",
      "Test doc 108: 69 sentences\n",
      "Test doc 109: 82 sentences\n",
      "Test doc 110: 65 sentences\n",
      "Test doc 111: 95 sentences\n",
      "Test doc 112: 115 sentences\n",
      "Test doc 113: 59 sentences\n",
      "Test doc 114: 172 sentences\n",
      "Test doc 115: 168 sentences\n",
      "Test doc 116: 157 sentences\n",
      "Test doc 117: 98 sentences\n",
      "Test doc 118: 138 sentences\n",
      "Test doc 119: 70 sentences\n",
      "Test doc 120: 96 sentences\n",
      "Test doc 121: 101 sentences\n",
      "Test doc 122: 174 sentences\n",
      "Test doc 123: 121 sentences\n",
      "Test doc 124: 55 sentences\n",
      "Test doc 125: 115 sentences\n",
      "Test doc 126: 87 sentences\n",
      "Test doc 127: 34 sentences\n",
      "Test doc 128: 99 sentences\n",
      "Test doc 129: 239 sentences\n",
      "Test doc 130: 67 sentences\n",
      "Test doc 131: 89 sentences\n",
      "Test doc 132: 91 sentences\n",
      "Test doc 133: 61 sentences\n",
      "Test doc 134: 110 sentences\n",
      "Test doc 135: 58 sentences\n",
      "Test doc 136: 50 sentences\n",
      "Test doc 137: 99 sentences\n",
      "Test doc 138: 80 sentences\n",
      "Test doc 139: 88 sentences\n",
      "Test doc 140: 33 sentences\n",
      "Test doc 141: 44 sentences\n",
      "Test doc 142: 41 sentences\n",
      "Test doc 143: 62 sentences\n",
      "Test doc 144: 80 sentences\n",
      "Test doc 145: 84 sentences\n",
      "Test doc 146: 58 sentences\n",
      "Test doc 147: 67 sentences\n",
      "Test doc 148: 133 sentences\n",
      "Test doc 149: 70 sentences\n",
      "Test doc 150: 139 sentences\n",
      "Test doc 151: 104 sentences\n",
      "Test doc 152: 114 sentences\n",
      "Test doc 153: 76 sentences\n",
      "Test doc 154: 140 sentences\n",
      "Test doc 155: 156 sentences\n",
      "Test doc 156: 107 sentences\n",
      "Test doc 157: 125 sentences\n",
      "Test doc 158: 69 sentences\n",
      "Test doc 159: 82 sentences\n",
      "Test doc 160: 65 sentences\n",
      "Test doc 161: 95 sentences\n",
      "Test doc 162: 115 sentences\n",
      "Test doc 163: 59 sentences\n",
      "Test doc 164: 172 sentences\n",
      "Test doc 165: 168 sentences\n",
      "Test doc 166: 157 sentences\n",
      "Test doc 167: 98 sentences\n",
      "Test doc 168: 138 sentences\n",
      "Test doc 169: 70 sentences\n",
      "Test doc 170: 96 sentences\n",
      "Test doc 171: 101 sentences\n",
      "Test doc 172: 174 sentences\n",
      "Test doc 173: 121 sentences\n",
      "Test doc 174: 55 sentences\n",
      "Test doc 175: 115 sentences\n",
      "Test doc 176: 87 sentences\n",
      "Test doc 177: 34 sentences\n",
      "Test doc 178: 99 sentences\n",
      "Test doc 179: 239 sentences\n",
      "Test doc 180: 67 sentences\n",
      "Test doc 181: 89 sentences\n",
      "Test doc 182: 91 sentences\n",
      "Test doc 183: 61 sentences\n",
      "Test doc 184: 110 sentences\n",
      "Test doc 185: 58 sentences\n",
      "Test doc 186: 50 sentences\n",
      "Test doc 187: 99 sentences\n",
      "Test doc 188: 80 sentences\n",
      "Test doc 189: 88 sentences\n",
      "Test doc 190: 77 sentences\n",
      "Test doc 191: 103 sentences\n",
      "Test doc 192: 80 sentences\n",
      "Test doc 193: 84 sentences\n",
      "Test doc 194: 125 sentences\n",
      "Test doc 195: 203 sentences\n",
      "Test doc 196: 139 sentences\n",
      "Test doc 197: 218 sentences\n",
      "Test doc 198: 76 sentences\n",
      "Test doc 199: 140 sentences\n",
      "Test doc 200: 263 sentences\n",
      "Test doc 201: 125 sentences\n",
      "Test doc 202: 151 sentences\n",
      "Test doc 203: 160 sentences\n",
      "Test doc 204: 174 sentences\n",
      "Test doc 205: 172 sentences\n",
      "Test doc 206: 168 sentences\n",
      "Test doc 207: 157 sentences\n",
      "Test doc 208: 236 sentences\n",
      "Test doc 209: 70 sentences\n",
      "Test doc 210: 197 sentences\n",
      "Test doc 211: 174 sentences\n",
      "Test doc 212: 121 sentences\n",
      "Test doc 213: 55 sentences\n",
      "Test doc 214: 202 sentences\n",
      "Test doc 215: 133 sentences\n",
      "Test doc 216: 239 sentences\n",
      "Test doc 217: 156 sentences\n",
      "Test doc 218: 152 sentences\n",
      "Test doc 219: 110 sentences\n",
      "Test doc 220: 58 sentences\n",
      "Test doc 221: 350 sentences\n",
      "Test doc 222: 311 sentences\n",
      "Test doc 223: 467 sentences\n",
      "Test doc 224: 434 sentences\n",
      "Test doc 225: 388 sentences\n",
      "Test doc 226: 426 sentences\n",
      "Test doc 227: 654 sentences\n",
      "Test doc 228: 405 sentences\n",
      "Test doc 229: 552 sentences\n",
      "Test doc 230: 439 sentences\n",
      "Test doc 231: 409 sentences\n",
      "Test doc 232: 50 sentences\n",
      "Test doc 233: 99 sentences\n",
      "Test doc 234: 80 sentences\n",
      "Test doc 235: 88 sentences\n",
      "Test doc 236: 33 sentences\n",
      "Test doc 237: 44 sentences\n",
      "Test doc 238: 41 sentences\n",
      "Test doc 239: 62 sentences\n",
      "Test doc 240: 80 sentences\n",
      "Test doc 241: 84 sentences\n",
      "Test doc 242: 58 sentences\n",
      "Test doc 243: 67 sentences\n",
      "Test doc 244: 133 sentences\n",
      "Test doc 245: 70 sentences\n",
      "Test doc 246: 139 sentences\n",
      "Test doc 247: 104 sentences\n",
      "Test doc 248: 114 sentences\n",
      "Test doc 249: 76 sentences\n",
      "Test doc 250: 140 sentences\n",
      "Test doc 251: 156 sentences\n",
      "Test doc 252: 107 sentences\n",
      "Test doc 253: 125 sentences\n",
      "Test doc 254: 69 sentences\n",
      "Test doc 255: 82 sentences\n",
      "Test doc 256: 65 sentences\n",
      "Test doc 257: 95 sentences\n",
      "Test doc 258: 115 sentences\n",
      "Test doc 259: 59 sentences\n",
      "Test doc 260: 172 sentences\n",
      "Test doc 261: 168 sentences\n",
      "Test doc 262: 157 sentences\n",
      "Test doc 263: 98 sentences\n",
      "Test doc 264: 138 sentences\n",
      "Test doc 265: 70 sentences\n",
      "Test doc 266: 96 sentences\n",
      "Test doc 267: 101 sentences\n",
      "Test doc 268: 174 sentences\n",
      "Test doc 269: 121 sentences\n",
      "Test doc 270: 55 sentences\n",
      "Test doc 271: 115 sentences\n",
      "Test doc 272: 87 sentences\n",
      "Test doc 273: 34 sentences\n",
      "Test doc 274: 99 sentences\n",
      "Test doc 275: 239 sentences\n",
      "Test doc 276: 67 sentences\n",
      "Test doc 277: 89 sentences\n",
      "Test doc 278: 91 sentences\n",
      "Test doc 279: 50 sentences\n",
      "Test doc 280: 99 sentences\n",
      "Test doc 281: 80 sentences\n",
      "Test doc 282: 88 sentences\n",
      "Test doc 283: 33 sentences\n",
      "Test doc 284: 44 sentences\n",
      "Test doc 285: 41 sentences\n",
      "Test doc 286: 62 sentences\n",
      "Test doc 287: 80 sentences\n",
      "Test doc 288: 84 sentences\n",
      "Test doc 289: 58 sentences\n",
      "Test doc 290: 67 sentences\n",
      "Test doc 291: 133 sentences\n",
      "Test doc 292: 70 sentences\n",
      "Test doc 293: 139 sentences\n",
      "Test doc 294: 104 sentences\n",
      "Test doc 295: 114 sentences\n",
      "Test doc 296: 76 sentences\n",
      "Test doc 297: 140 sentences\n",
      "Test doc 298: 156 sentences\n",
      "Test doc 299: 107 sentences\n",
      "Test doc 300: 125 sentences\n",
      "Test doc 301: 528 sentences\n",
      "Test doc 302: 1337 sentences\n",
      "Test doc 303: 692 sentences\n",
      "Test doc 304: 964 sentences\n",
      "Test doc 305: 651 sentences\n",
      "Test doc 306: 1117 sentences\n",
      "Test doc 307: 1279 sentences\n",
      "Test doc 308: 1274 sentences\n",
      "Test doc 309: 479 sentences\n",
      "Test doc 310: 77 sentences\n",
      "Test doc 311: 99 sentences\n",
      "Test doc 312: 169 sentences\n",
      "Test doc 313: 183 sentences\n",
      "Test doc 314: 66 sentences\n",
      "Test doc 315: 197 sentences\n",
      "Test doc 316: 87 sentences\n",
      "Test doc 317: 372 sentences\n",
      "Test doc 318: 125 sentences\n",
      "Test doc 319: 162 sentences\n",
      "Test doc 320: 134 sentences\n",
      "Test doc 321: 88 sentences\n",
      "Test doc 322: 106 sentences\n",
      "Test doc 323: 66 sentences\n",
      "Test doc 324: 214 sentences\n",
      "Test doc 325: 63 sentences\n",
      "Test doc 326: 40 sentences\n",
      "Test doc 327: 65 sentences\n",
      "Test doc 328: 119 sentences\n",
      "Test doc 329: 125 sentences\n",
      "Test doc 330: 178 sentences\n",
      "Test doc 331: 174 sentences\n",
      "Test doc 332: 102 sentences\n",
      "Test doc 333: 148 sentences\n",
      "Test doc 334: 43 sentences\n",
      "Test doc 335: 241 sentences\n",
      "Test doc 336: 78 sentences\n",
      "Test doc 337: 133 sentences\n",
      "Test doc 338: 162 sentences\n",
      "Test doc 339: 138 sentences\n",
      "Test doc 340: 218 sentences\n",
      "Test doc 341: 216 sentences\n",
      "Test doc 342: 181 sentences\n",
      "Test doc 343: 297 sentences\n",
      "Test doc 344: 89 sentences\n",
      "Test doc 345: 158 sentences\n",
      "Test doc 346: 176 sentences\n",
      "Test doc 347: 81 sentences\n",
      "Test doc 348: 105 sentences\n",
      "Test doc 349: 150 sentences\n",
      "Test doc 350: 302 sentences\n",
      "Test doc 351: 206 sentences\n",
      "Test doc 352: 205 sentences\n",
      "Test doc 353: 156 sentences\n",
      "Test doc 354: 74 sentences\n",
      "Test doc 355: 130 sentences\n",
      "Test doc 356: 31 sentences\n",
      "Test doc 357: 91 sentences\n",
      "Test doc 358: 92 sentences\n",
      "Test doc 359: 219 sentences\n",
      "Test doc 360: 162 sentences\n",
      "Test doc 361: 245 sentences\n",
      "Test doc 362: 304 sentences\n",
      "Test doc 363: 85 sentences\n",
      "Test doc 364: 87 sentences\n",
      "Test doc 365: 189 sentences\n",
      "Test doc 366: 118 sentences\n",
      "Test doc 367: 77 sentences\n",
      "Test doc 368: 99 sentences\n",
      "Test doc 369: 169 sentences\n",
      "Test doc 370: 183 sentences\n",
      "Test doc 371: 66 sentences\n",
      "Test doc 372: 197 sentences\n",
      "Test doc 373: 87 sentences\n",
      "Test doc 374: 372 sentences\n",
      "Test doc 375: 125 sentences\n",
      "Test doc 376: 162 sentences\n",
      "Test doc 377: 134 sentences\n",
      "Test doc 378: 88 sentences\n",
      "Test doc 379: 106 sentences\n",
      "Test doc 380: 66 sentences\n",
      "Test doc 381: 214 sentences\n",
      "Test doc 382: 63 sentences\n",
      "Test doc 383: 40 sentences\n",
      "Test doc 384: 65 sentences\n",
      "Test doc 385: 119 sentences\n",
      "Test doc 386: 125 sentences\n",
      "Test doc 387: 178 sentences\n",
      "Test doc 388: 174 sentences\n",
      "Test doc 389: 102 sentences\n",
      "Test doc 390: 148 sentences\n",
      "Test doc 391: 43 sentences\n",
      "Test doc 392: 241 sentences\n",
      "Test doc 393: 78 sentences\n",
      "Test doc 394: 133 sentences\n",
      "Test doc 395: 162 sentences\n",
      "Test doc 396: 138 sentences\n",
      "Test doc 397: 218 sentences\n",
      "Test doc 398: 216 sentences\n",
      "Test doc 399: 181 sentences\n",
      "Test doc 400: 297 sentences\n",
      "Test doc 401: 89 sentences\n",
      "Test doc 402: 158 sentences\n",
      "Test doc 403: 176 sentences\n",
      "Test doc 404: 81 sentences\n",
      "Test doc 405: 105 sentences\n",
      "Test doc 406: 150 sentences\n",
      "Test doc 407: 302 sentences\n",
      "Test doc 408: 206 sentences\n",
      "Test doc 409: 205 sentences\n",
      "Test doc 410: 156 sentences\n",
      "Test doc 411: 74 sentences\n",
      "Test doc 412: 130 sentences\n",
      "Test doc 413: 31 sentences\n",
      "Test doc 414: 91 sentences\n",
      "Test doc 415: 92 sentences\n",
      "Test doc 416: 219 sentences\n",
      "Test doc 417: 162 sentences\n",
      "Test doc 418: 245 sentences\n",
      "Test doc 419: 304 sentences\n",
      "Test doc 420: 85 sentences\n",
      "Test doc 421: 87 sentences\n",
      "Test doc 422: 189 sentences\n",
      "Test doc 423: 118 sentences\n",
      "Test doc 424: 528 sentences\n",
      "Test doc 425: 722 sentences\n",
      "Test doc 426: 895 sentences\n",
      "Test doc 427: 866 sentences\n",
      "Test doc 428: 943 sentences\n",
      "Test doc 429: 912 sentences\n",
      "Test doc 430: 504 sentences\n",
      "Test doc 431: 763 sentences\n",
      "Test doc 432: 687 sentences\n",
      "Test doc 433: 473 sentences\n",
      "Test doc 434: 1028 sentences\n",
      "Test doc 435: 110 sentences\n",
      "Test doc 436: 114 sentences\n",
      "Test doc 437: 122 sentences\n",
      "Test doc 438: 219 sentences\n",
      "Test doc 439: 173 sentences\n",
      "Test doc 440: 122 sentences\n",
      "Test doc 441: 94 sentences\n",
      "Test doc 442: 129 sentences\n",
      "Test doc 443: 109 sentences\n",
      "Test doc 444: 148 sentences\n",
      "Test doc 445: 144 sentences\n",
      "Test doc 446: 213 sentences\n",
      "Test doc 447: 44 sentences\n",
      "Test doc 448: 189 sentences\n",
      "Test doc 449: 44 sentences\n",
      "Test doc 450: 72 sentences\n",
      "Test doc 451: 127 sentences\n",
      "Test doc 452: 114 sentences\n",
      "Test doc 453: 134 sentences\n",
      "Test doc 454: 55 sentences\n",
      "Test doc 455: 150 sentences\n",
      "Test doc 456: 62 sentences\n",
      "Test doc 457: 166 sentences\n",
      "Test doc 458: 68 sentences\n",
      "Test doc 459: 149 sentences\n",
      "Test doc 460: 106 sentences\n",
      "Test doc 461: 110 sentences\n",
      "Test doc 462: 116 sentences\n",
      "Test doc 463: 135 sentences\n",
      "Test doc 464: 164 sentences\n",
      "Test doc 465: 188 sentences\n",
      "Test doc 466: 137 sentences\n",
      "Test doc 467: 198 sentences\n",
      "Test doc 468: 239 sentences\n",
      "Test doc 469: 212 sentences\n",
      "Test doc 470: 256 sentences\n",
      "Test doc 471: 139 sentences\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest doc \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m sentences\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i, \u001b[38;5;28mlen\u001b[39m(test_chapters_sentences[i])))\n\u001b[1;32m      5\u001b[0m summary_length_limit \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mceil(test_chapters_length[i] \u001b[38;5;241m*\u001b[39m average_summary_length_ratio)\n\u001b[0;32m----> 7\u001b[0m temp_sentences_embedding \u001b[38;5;241m=\u001b[39m \u001b[43msentence_embedding_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_chapters_sentences\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#print(type(temp_sentences_embedding))\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#print(temp_sentences_embedding.shape)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m cosine_scores \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mcos_sim(temp_sentences_embedding, temp_sentences_embedding)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:165\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    162\u001b[0m features \u001b[38;5;241m=\u001b[39m batch_to_device(features, device)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 165\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    168\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:66\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m     64\u001b[0m     trans_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 66\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     69\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m: output_tokens, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1013\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1015\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1016\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1017\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[0;32m-> 1022\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1035\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:612\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    603\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    604\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    605\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    609\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    610\u001b[0m     )\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 612\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    487\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    419\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 427\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:355\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    352\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m+\u001b[39m attention_mask\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[1;32m    359\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_probs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:1843\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1841\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1842\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1843\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1845\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test summary generation\n",
    "test_generated_summaries = []\n",
    "for i in range(len(test_chapters)):\n",
    "    print(\"Test doc {}: {} sentences\".format(i, len(test_chapters_sentences[i])))\n",
    "    summary_length_limit = math.ceil(test_chapters_length[i] * average_summary_length_ratio)\n",
    "\n",
    "    pairwise_sentence_scores = np.zeros((len(test_chapters_sentences[i]), len(test_chapters_sentences[i])))\n",
    "    for j in range(len(test_chapters_sentences[i])):\n",
    "        for k in range(len(test_chapters_sentences[i])):\n",
    "            pairwise_sentence_scores[j, k] = next_sentence_score(test_chapters_sentences[i][j], test_chapters_sentences[i][k])\n",
    "    \n",
    "    sentence_sum_similarity = [(0, j) for j in range(len(test_chapters_sentences[i]))]\n",
    "    for j in range(len(test_chapters_sentences[i])):\n",
    "        for k in range(len(test_chapters_sentences[i])):\n",
    "            #print(\"j = {} - k = {}\".format(j, k))\n",
    "            sentence_sum_similarity[j] = (sentence_sum_similarity[j][0] + pairwise_sentence_scores[j][k], j)\n",
    "    sentence_sum_similarity.sort(reverse = True)\n",
    "    generated_summary_length = 0\n",
    "    chosen_sentences_index = []\n",
    "    for j in range(len(test_chapters_sentences[i])):\n",
    "        chosen_sentences_index.append(sentence_sum_similarity[j][1])\n",
    "        generated_summary_length += len(' '.join(test_chapters_sentences[i][sentence_sum_similarity[j][1]].split(\" \")).split())\n",
    "        if generated_summary_length >= summary_length_limit:\n",
    "            break\n",
    "    chosen_sentences_index.sort()\n",
    "    generated_summary = \"\"\n",
    "    for j in range(len(chosen_sentences_index)):\n",
    "        if j > 0:\n",
    "            generated_summary += \" \"\n",
    "        generated_summary += test_chapters_sentences[i][chosen_sentences_index[j]]\n",
    "    test_generated_summaries.append(generated_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec7114c-12c8-48ad-b2df-dad75bd242d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reference summaries\n",
    "test_reference_summaries = test['summary_text'].tolist()\n",
    "test_reference_summaries = [test_reference_summaries[i].replace(\"\\n\", \" \") for i in range(len(test_reference_summaries))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4d1ee9-f644-486b-bf11-2acb8eb2963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ROUGE score\n",
    "test_rouge1 = 0\n",
    "test_rouge2 = 0\n",
    "test_rougeL = 0\n",
    "for i in range(len(test_generated_summaries)):\n",
    "    print(i)\n",
    "    scores = rougescorer.score(test_reference_summaries[i], test_generated_summaries[i])\n",
    "    for key in scores:\n",
    "        print(\"{}: {}\".format(key, scores[key]))\n",
    "        if key == \"rouge1\":\n",
    "            test_rouge1 += scores[key][2] # take fmeasure value\n",
    "        elif key == \"rouge2\":\n",
    "            test_rouge2 += scores[key][2] # take fmeasure value\n",
    "        else:\n",
    "            test_rougeL += scores[key][2] # take fmeasure value\n",
    "test_rouge1 /= len(test_generated_summaries)\n",
    "test_rouge2 /= len(test_generated_summaries)\n",
    "test_rougeL /= len(test_generated_summaries)\n",
    "print(\"Test: rouge1 = {}, rouge2 = {}, rougeL = {}\".format(test_rouge1, test_rouge2, test_rougeL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aa2d64-9419-4245-920e-52d8267685a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test BERT score\n",
    "average_bertscore_F1 = 0\n",
    "\n",
    "for i in range(len(test_generated_summaries)):\n",
    "    print(\"Test doc {}:\".format(i))\n",
    "    \n",
    "    ref_summary = copy.deepcopy(test_reference_summaries[i])\n",
    "    candidate_summary = copy.deepcopy(test_generated_summaries[i])\n",
    "\n",
    "    # Method 1: Split by sentence\n",
    "    ref_sentences = sent_tokenize(ref_summary)\n",
    "    candidate_sentences = sent_tokenize(candidate_summary)\n",
    "    chapter_bertscore_F1 = 0\n",
    "    for j in range(len(ref_sentences)):\n",
    "        ref_summary_split = ref_sentences[j]\n",
    "        max_candidate_score = -1\n",
    "        for k in range(len(candidate_sentences)):\n",
    "            candidate_summary_split = candidate_sentences[k]\n",
    "            P, R, F1 = bertscorer.score([candidate_summary_split], [ref_summary_split])\n",
    "            #print(\"Candidate length = {}, reference length = {}\".format(len(word_tokenize(candidate_summary_split)), len(word_tokenize(ref_summary_split))))\n",
    "            max_candidate_score = max(max_candidate_score, F1.mean().item())\n",
    "        chapter_bertscore_F1 += max_candidate_score\n",
    "    chapter_bertscore_F1 /= len(ref_sentences)\n",
    "    average_bertscore_F1 += chapter_bertscore_F1\n",
    "    print(\"Chapter BERTScore F1 = {}\".format(chapter_bertscore_F1))\n",
    "\n",
    "    '''\n",
    "    # Method 2: Split by n words\n",
    "    ref_tokenized = word_tokenize(ref_summary)\n",
    "    candidate_tokenized = word_tokenize(candidate_summary)\n",
    "    split_length = 250\n",
    "    ref_pointer = 0\n",
    "    ref_split_count = 0\n",
    "    chapter_bertscore_F1 = 0\n",
    "    while ref_pointer < len(ref_tokenized):\n",
    "        ref_summary_split = ' '.join(ref_tokenized[ref_pointer:min(len(ref_tokenized), ref_pointer + split_length)])\n",
    "        max_candidate_score = -1\n",
    "        candidate_pointer = 0\n",
    "        while candidate_pointer < len(candidate_tokenized):\n",
    "            candidate_summary_split = ' '.join(candidate_tokenized[candidate_pointer:min(len(candidate_tokenized), candidate_pointer + split_length)])\n",
    "            P, R, F1 = bertscorer.score([candidate_summary_split], [ref_summary_split])\n",
    "            #print(\"Candidate length = {}, reference length = {}\".format(len(word_tokenize(candidate_summary_split)), len(word_tokenize(ref_summary_split))))\n",
    "            max_candidate_score = max(max_candidate_score, F1.mean().item())\n",
    "            candidate_pointer += split_length\n",
    "        chapter_bertscore_F1 += max_candidate_score\n",
    "        ref_pointer += split_length\n",
    "        ref_split_count += 1\n",
    "    chapter_bertscore_F1 /= ref_split_count\n",
    "    average_bertscore_F1 += chapter_bertscore_F1\n",
    "    print(\"Chapter BERTScore F1 = {}\".format(chapter_bertscore_F1))\n",
    "    '''\n",
    "average_bertscore_F1 /= len(test_generated_summaries)\n",
    "print(\"Test average BERTscore F1 = {}\".format(average_bertscore_F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a9feed-dcea-4e27-b01b-db97fb93e76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test summaQA\n",
    "#srcs = test_chapters[:len(test_generated_summaries)]\n",
    "#gens = test_generated_summaries\n",
    "#srcs = [' '.join(' '.join(srcs[i].split(\" \")).split()[:300]) for i in range(len(srcs))]\n",
    "#gens = [' '.join(' '.join(gens[i].split(\" \")).split()[:300]) for i in range(len(gens))]\n",
    "#evaluate_corpus(srcs, gens)\n",
    "\n",
    "average_summaqa_prob = 0\n",
    "average_summaqa_F1 = 0\n",
    "\n",
    "for i in range(len(test_generated_summaries)):\n",
    "    print(\"Test doc {}:\".format(i))\n",
    "    \n",
    "    chapter_content = copy.deepcopy(test_chapters[i])\n",
    "    candidate_summary = copy.deepcopy(test_generated_summaries[i])\n",
    "\n",
    "    # Method 1: Split by sentence\n",
    "    chapter_sentences = test_chapters_sentences[i]\n",
    "    candidate_sentences = sent_tokenize(candidate_summary)\n",
    "    chapter_split_count = 0\n",
    "    chapter_summaqa_prob = 0\n",
    "    chapter_summaqa_F1 = 0\n",
    "    for j in range(len(chapter_sentences)):\n",
    "        chapter_summary_split = chapter_sentences[j]\n",
    "        article = chapter_summary_split\n",
    "        masked_questions, answer_spans = question_generator.get_questions(article)\n",
    "        if len(masked_questions) == 0:\n",
    "            continue\n",
    "        chapter_split_count += 1\n",
    "        print(\"Chapter split #{}:\".format(j))\n",
    "        print(chapter_summary_split)\n",
    "        print(\"Questions:\")\n",
    "        print(masked_questions)\n",
    "        print(\"Answers:\")\n",
    "        print(answer_spans)\n",
    "        max_candidate_F1 = 0\n",
    "        max_candidate_prob = 0\n",
    "        #chapter_split_sum_F1 = 0\n",
    "        #chapter_split_sum_prob = 0\n",
    "        for k in range(len(candidate_sentences)):\n",
    "            candidate_summary_split = candidate_sentences[k]\n",
    "            summaqa_scores = qa_metric.compute(masked_questions, answer_spans, candidate_summary_split)\n",
    "            max_candidate_F1 = max(max_candidate_F1, summaqa_scores['avg_fscore'])\n",
    "            max_candidate_prob = max(max_candidate_prob, summaqa_scores['avg_prob'])\n",
    "            #chapter_split_sum_F1 += summaqa_scores['avg_fscore']\n",
    "            #chapter_split_sum_prob += summaqa_scores['avg_prob']\n",
    "            print(\"Candidate split #{} (Chapter split length = {}, candidate length = {}): SummaQA score = {}\".format(k, len(word_tokenize(chapter_summary_split)), len(word_tokenize(candidate_summary_split)), summaqa_scores))\n",
    "            print(candidate_summary_split)\n",
    "        print(\"Chapter split max prob = {}\".format(max_candidate_prob))\n",
    "        print(\"Chapter split max F1 = {}\".format(max_candidate_F1))\n",
    "        #print(\"Chapter split sum prob = {}\".format(chapter_split_sum_prob))\n",
    "        #print(\"Chapter split sum F1 = {}\".format(chapter_split_sum_F1))\n",
    "        chapter_summaqa_prob += max_candidate_prob\n",
    "        chapter_summaqa_F1 += max_candidate_F1\n",
    "        #chapter_summaqa_prob += chapter_split_sum_prob\n",
    "        #chapter_summaqa_F1 += chapter_split_sum_F1\n",
    "    if chapter_split_count > 0:\n",
    "        chapter_summaqa_prob /= chapter_split_count\n",
    "        chapter_summaqa_F1 /= chapter_split_count\n",
    "    else:\n",
    "        chapter_summaqa_prob = 1\n",
    "        chapter_summaqa_F1 = 1\n",
    "    average_summaqa_prob += chapter_summaqa_prob\n",
    "    average_summaqa_F1 += chapter_summaqa_F1\n",
    "    print(\"Chapter SummaQA prob = {}\".format(chapter_summaqa_prob))\n",
    "    print(\"Chapter SummaQA F1 = {}\".format(chapter_summaqa_F1))\n",
    "    \n",
    "    '''\n",
    "    # Method 2: Split by n words\n",
    "    chapter_tokenized = word_tokenize(chapter_content)\n",
    "    candidate_tokenized = word_tokenize(candidate_summary)\n",
    "    split_length = 250\n",
    "    chapter_pointer = 0\n",
    "    chapter_split_count = 0\n",
    "    chapter_summaqa_prob = 0\n",
    "    chapter_summaqa_F1 = 0\n",
    "    while chapter_pointer < len(chapter_tokenized):\n",
    "        chapter_summary_split = ' '.join(chapter_tokenized[chapter_pointer:min(len(chapter_tokenized), chapter_pointer + split_length)])\n",
    "        article = chapter_summary_split\n",
    "        masked_questions, answer_spans = question_generator.get_questions(article)\n",
    "        print(\"Chapter split #{}:\".format(chapter_split_count))\n",
    "        print(chapter_summary_split)\n",
    "        print(\"Questions:\")\n",
    "        print(masked_questions)\n",
    "        print(\"Answers:\")\n",
    "        print(answer_spans)\n",
    "        #max_candidate_score = -1\n",
    "        chapter_split_sum_F1 = 0\n",
    "        chapter_split_sum_prob = 0\n",
    "        candidate_pointer = 0\n",
    "        while candidate_pointer < len(candidate_tokenized):\n",
    "            candidate_summary_split = ' '.join(candidate_tokenized[candidate_pointer:min(len(candidate_tokenized), candidate_pointer + split_length)])\n",
    "            summaqa_scores = qa_metric.compute(masked_questions, answer_spans, candidate_summary_split)\n",
    "            #max_candidate_score = max(max_candidate_score, summaqa_scores['avg_fscore'])\n",
    "            chapter_split_sum_F1 += summaqa_scores['avg_fscore']\n",
    "            chapter_split_sum_prob += summaqa_scores['avg_prob']\n",
    "            print(\"Candidate split #{} (Chapter split length = {}, candidate length = {}): SummaQA score = {}\".format(math.floor(candidate_pointer / split_length), len(word_tokenize(chapter_summary_split)), len(word_tokenize(candidate_summary_split)), summaqa_scores))\n",
    "            print(candidate_summary_split)\n",
    "            candidate_pointer += split_length\n",
    "        print(\"Chapter split sum prob = {}\".format(chapter_split_sum_prob))\n",
    "        print(\"Chapter split sum F1 = {}\".format(chapter_split_sum_F1))\n",
    "        chapter_summaqa_prob += chapter_split_sum_prob\n",
    "        chapter_summaqa_F1 += chapter_split_sum_F1\n",
    "        #chapter_summaqa_F1 += max_candidate_score\n",
    "        chapter_pointer += split_length\n",
    "        chapter_split_count += 1\n",
    "    chapter_summaqa_prob /= chapter_split_count\n",
    "    chapter_summaqa_F1 /= chapter_split_count\n",
    "    average_summaqa_prob += chapter_summaqa_prob\n",
    "    average_summaqa_F1 += chapter_summaqa_F1\n",
    "    print(\"Chapter SummaQA prob = {}\".format(chapter_summaqa_prob))\n",
    "    print(\"Chapter SummaQA F1 = {}\".format(chapter_summaqa_F1))\n",
    "    '''\n",
    "\n",
    "average_summaqa_prob /= len(test_generated_summaries)\n",
    "average_summaqa_F1 /= len(test_generated_summaries)\n",
    "print(\"Test average SummaQA prob = {}\".format(average_summaqa_prob))\n",
    "print(\"Test average SummaQA F1 = {}\".format(average_summaqa_F1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a297f2a3-c316-4551-85d2-db5b14310f16",
   "metadata": {},
   "source": [
    "## End of [6']: Run test & evaluate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

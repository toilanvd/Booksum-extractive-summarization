{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df16a5bf-b353-4b65-b6b6-ccf7710b25e3",
   "metadata": {},
   "source": [
    "# MCS-GA-One-and-Two-Words-tfidf_CNN-Dailymail model for text summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d181ac9-c187-4a40-9693-a0118eab5b53",
   "metadata": {},
   "source": [
    "## Start of [1]: Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179df10c-d582-4374-9dc7-529a4bb2af32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36db0d43-d3d1-4d87-9e43-efafa9f33bc5",
   "metadata": {},
   "source": [
    "Show training data head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126813b5-0cfb-45c0-9857-d92cbaf52ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "val = pd.read_csv(\"Dataset/cnn_dailymail/validation.csv\")\n",
    "test = pd.read_csv(\"Dataset/cnn_dailymail/test.csv\")\n",
    "val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac330a27-a92b-40f1-bbfe-1de78ba57ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val.iloc[0]['article'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e638c12c-2e6c-40f3-8b5e-75839ca216d3",
   "metadata": {},
   "source": [
    "## End of [1]: Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310dbb9f-a741-4c49-8b05-414a1ff8dcbc",
   "metadata": {},
   "source": [
    "## Start of [2]: Span recognition (sentence splitting in this model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9d47bb-3b15-4a11-a131-cd057934a4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad797c05-ed8f-45f3-8ea2-64fef1d01113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "sent_tokenize(\"Hello! Hello world... It's true that Mr.Ryo is here? That's true!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d16969b-79b5-4c2b-aa9d-2488207a4f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_chapters = val['article'].tolist()\n",
    "val_chapters_paragraphs = [val_chapters[i].split(\"\\n\\n\") for i in range(len(val_chapters))]\n",
    "for i in range(len(val_chapters_paragraphs)):\n",
    "    val_chapters_paragraphs[i] = [val_chapters_paragraphs[i][j].replace(\"\\n\", \" \") for j in range(len(val_chapters_paragraphs[i]))]\n",
    "val_chapters = [val_chapters[i].replace(\"\\n\", \" \") for i in range(len(val_chapters))]\n",
    "test_chapters = test['article'].tolist()\n",
    "test_chapters_paragraphs = [test_chapters[i].split(\"\\n\\n\") for i in range(len(test_chapters))]\n",
    "for i in range(len(test_chapters_paragraphs)):\n",
    "    test_chapters_paragraphs[i] = [test_chapters_paragraphs[i][j].replace(\"\\n\", \" \") for j in range(len(test_chapters_paragraphs[i]))]\n",
    "test_chapters = [test_chapters[i].replace(\"\\n\", \" \") for i in range(len(test_chapters))]\n",
    "\n",
    "print(val_chapters_paragraphs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b145b468-a7ea-4d8a-b76e-8e5147860cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_chapters_sentences = [sent_tokenize(val_chapters[i]) for i in range(len(val_chapters))]\n",
    "test_chapters_sentences = [sent_tokenize(test_chapters[i]) for i in range(len(test_chapters))]\n",
    "\n",
    "val_chapters_paragraphs_sentences = []\n",
    "for i in range(len(val_chapters_paragraphs)): # chapter i\n",
    "    chapter_list = []\n",
    "    for j in range(len(val_chapters_paragraphs[i])): # paragraph j\n",
    "        chapter_list.append(sent_tokenize(val_chapters_paragraphs[i][j]))\n",
    "    val_chapters_paragraphs_sentences.append(chapter_list)\n",
    "test_chapters_paragraphs_sentences = []\n",
    "for i in range(len(test_chapters_paragraphs)): # chapter i\n",
    "    chapter_list = []\n",
    "    for j in range(len(test_chapters_paragraphs[i])): # paragraph j\n",
    "        chapter_list.append(sent_tokenize(test_chapters_paragraphs[i][j]))\n",
    "    test_chapters_paragraphs_sentences.append(chapter_list)\n",
    "\n",
    "print(val_chapters_paragraphs_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9083aeb-54c5-4ccf-96e1-bfdfd6c4f1f3",
   "metadata": {},
   "source": [
    "## End of [2]: Span recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bb0b93-563f-4abc-827d-cccc663cf5b9",
   "metadata": {},
   "source": [
    "## Start of [3]: (BERT + cosine) score function for sentence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b462624b-194e-4b1d-b98a-18e07b93c53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "sentence_embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2', device = device)  # multi-language model  \n",
    "\n",
    "def sentence_similarity(sentence_A, sentence_B):\n",
    "    temp_sentence_A = [sentence_A]\n",
    "    temp_embedding_A = sentence_embedding_model.encode(temp_sentence_A, convert_to_tensor=False)\n",
    "    temp_sentence_B = [sentence_B]\n",
    "    temp_embedding_B = sentence_embedding_model.encode(temp_sentence_B, convert_to_tensor=False)\n",
    "    cosine_score = util.cos_sim(temp_embedding_A, temp_embedding_B)\n",
    "    return cosine_score[0][0].item()\n",
    "\n",
    "print(val_chapters_sentences[0][2])\n",
    "print(val_chapters_sentences[0][3])\n",
    "print(sentence_similarity(val_chapters_sentences[0][2], val_chapters_sentences[0][3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09c052a-85b1-4f60-b13c-16b8bc851a34",
   "metadata": {},
   "source": [
    "## End of [3]: (BERT + cosine) score function for sentence similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50387011-e37e-4d2a-afc2-3fc0bcdf876c",
   "metadata": {},
   "source": [
    "## Start of [5']: Metrics preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc37c500-4242-4b80-b3a0-6ad33e7c5353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation reference summaries\n",
    "val_reference_summaries = val['highlights'].tolist()\n",
    "val_reference_summaries = [val_reference_summaries[i].replace(\"\\n\", \" \") for i in range(len(val_reference_summaries))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd7a6a8-9021-4e9c-a641-e832b2566c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average summary length vs chapter length ratio\n",
    "average_summary_length_ratio = 0\n",
    "for i in range(len(val_chapters)):\n",
    "    average_summary_length_ratio += len(word_tokenize(val_reference_summaries[i])) / len(word_tokenize(val_chapters[i]))\n",
    "average_summary_length_ratio /= len(val_chapters)\n",
    "#average_summary_length_ratio = 0.1\n",
    "print(average_summary_length_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81759ca9-43f8-4f76-a4b6-e9eb0a673c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "EPS = 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a413c054-9277-4569-a2ec-a6e6b7728660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "rougescorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1567d40e-7411-456d-bfd9-c821116aa93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_A = \"1 I love machine learning\"\n",
    "sent_B = \"1 think love i machine learning..\"\n",
    "print(rougescorer.score(sent_B, sent_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f8383d-de4e-4e83-8bde-630c17e6cf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert_score\n",
    "from bert_score import BERTScorer\n",
    "\n",
    "bertscorer = BERTScorer(lang='en', rescale_with_baseline=True) # Default using 'roberta-large' model\n",
    "#bertscorer = BERTScorer(model_type='xlnet-large-cased', rescale_with_baseline=True, lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8928a855-f56d-4b51-ad69-b9fc422f7c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from summaqa import evaluate_corpus\n",
    "from summaqa import QG_masked\n",
    "question_generator = QG_masked()\n",
    "from summaqa import QA_Metric\n",
    "qa_metric = QA_Metric()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d104c54-16c0-4c8f-81b8-a9d064bc0f67",
   "metadata": {},
   "source": [
    "## End of [5']: Metrics preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dde843-5d32-4e0e-a37e-4c1e216b8429",
   "metadata": {},
   "source": [
    "## Start of [6']: Run test & evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec7114c-12c8-48ad-b2df-dad75bd242d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reference summaries\n",
    "test_reference_summaries = test['highlights'].tolist()\n",
    "test_reference_summaries = [test_reference_summaries[i].replace(\"\\n\", \" \") for i in range(len(test_reference_summaries))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d999411-d0c4-4c56-ab13-71f4158d50cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test summary generation\n",
    "test_generated_summaries = []\n",
    "for i in range(len(test_chapters)):\n",
    "    num_sentences = len(test_chapters_sentences[i])\n",
    "    print(\"Test doc {}: {} sentences\".format(i, len(test_chapters_sentences[i])))\n",
    "    summary_length_limit = math.ceil(len(word_tokenize(test_chapters[i])) * average_summary_length_ratio)\n",
    "    sentences_length = [len(word_tokenize(test_chapters_sentences[i][j])) for j in range(num_sentences)]\n",
    "    #summary_length_limit = sum(sentences_length)\n",
    "    \n",
    "    inputFile = open(\"MCS-GA-One-and-Two-Words-tfidf-input.txt\", \"w\")\n",
    "    print(\"{} {}\".format(num_sentences, summary_length_limit), file = inputFile)\n",
    "    for j in range(num_sentences):\n",
    "        print(\"{} \".format(sentences_length[j]), end = \"\", file = inputFile)\n",
    "    print(\"\", file = inputFile)\n",
    "    for j in range(num_sentences):\n",
    "        print(\"{}\".format(test_chapters_sentences[i][j]), file = inputFile)\n",
    "    inputFile.close()\n",
    "\n",
    "    os.system(\"./MCS-GA-One-and-Two-Words-tfidf\")\n",
    "    chosen_sentences_index = []\n",
    "    outputFile = open(\"MCS-GA-One-and-Two-Words-tfidf-output.txt\", \"r\")\n",
    "    num_lines = int(outputFile.readline())\n",
    "    for j in range(num_lines):\n",
    "        temp_index = int(outputFile.readline())\n",
    "        chosen_sentences_index.append(temp_index)\n",
    "    current_profit = float(outputFile.readline())\n",
    "    print(\"Profit = {0:.9f}\".format(current_profit))\n",
    "    outputFile.close()\n",
    "    \n",
    "    chosen_sentences_index.sort()\n",
    "    generated_summary = \"\"\n",
    "    for j in range(len(chosen_sentences_index)):\n",
    "        if j > 0:\n",
    "            generated_summary += \" \"\n",
    "        generated_summary += test_chapters_sentences[i][chosen_sentences_index[j]]\n",
    "    test_generated_summaries.append(generated_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8027baa9-946c-4575-87ee-8046b6aba325",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_generated_summaries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4d1ee9-f644-486b-bf11-2acb8eb2963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ROUGE score\n",
    "test_rouge1 = 0\n",
    "test_rouge2 = 0\n",
    "test_rougeL = 0\n",
    "for i in range(len(test_generated_summaries)):\n",
    "    print(i)\n",
    "    scores = rougescorer.score(test_reference_summaries[i], test_generated_summaries[i])\n",
    "    for key in scores:\n",
    "        print(\"{}: {}\".format(key, scores[key]))\n",
    "        if key == \"rouge1\":\n",
    "            test_rouge1 += scores[key][2] # take fmeasure value\n",
    "        elif key == \"rouge2\":\n",
    "            test_rouge2 += scores[key][2] # take fmeasure value\n",
    "        else:\n",
    "            test_rougeL += scores[key][2] # take fmeasure value\n",
    "test_rouge1 /= len(test_generated_summaries)\n",
    "test_rouge2 /= len(test_generated_summaries)\n",
    "test_rougeL /= len(test_generated_summaries)\n",
    "print(\"Test: rouge1 = {}, rouge2 = {}, rougeL = {}\".format(test_rouge1, test_rouge2, test_rougeL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318a221a-1cd4-43be-9785-ac13497862aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print test result to file\n",
    "test_result_file = open(\"MCS-GA-One-and-Two-Words-tfidf_CNN-Dailymail_test-generated-result.txt\", \"w\")\n",
    "for i in range(len(test_generated_summaries)):\n",
    "    print(test_generated_summaries[i], file = test_result_file)\n",
    "test_result_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aa2d64-9419-4245-920e-52d8267685a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test BERT score\n",
    "average_bertscore_F1 = 0\n",
    "\n",
    "for i in range(len(test_generated_summaries)):\n",
    "    print(\"Test doc {}:\".format(i))\n",
    "    \n",
    "    ref_summary = copy.deepcopy(test_reference_summaries[i])\n",
    "    candidate_summary = copy.deepcopy(test_generated_summaries[i])\n",
    "\n",
    "    # Method 1: Split by sentence\n",
    "    ref_sentences = sent_tokenize(ref_summary)\n",
    "    candidate_sentences = sent_tokenize(candidate_summary)\n",
    "    chapter_bertscore_F1 = 0\n",
    "    for j in range(len(ref_sentences)):\n",
    "        ref_summary_split = ref_sentences[j]\n",
    "        max_candidate_score = -1\n",
    "        for k in range(len(candidate_sentences)):\n",
    "            candidate_summary_split = candidate_sentences[k]\n",
    "            P, R, F1 = bertscorer.score([candidate_summary_split], [ref_summary_split])\n",
    "            #print(\"Candidate length = {}, reference length = {}\".format(len(word_tokenize(candidate_summary_split)), len(word_tokenize(ref_summary_split))))\n",
    "            max_candidate_score = max(max_candidate_score, F1.mean().item())\n",
    "        chapter_bertscore_F1 += max_candidate_score\n",
    "    chapter_bertscore_F1 /= len(ref_sentences)\n",
    "    average_bertscore_F1 += chapter_bertscore_F1\n",
    "    print(\"Chapter BERTScore F1 = {}\".format(chapter_bertscore_F1))\n",
    "\n",
    "    '''\n",
    "    # Method 2: Split by n words\n",
    "    ref_tokenized = word_tokenize(ref_summary)\n",
    "    candidate_tokenized = word_tokenize(candidate_summary)\n",
    "    split_length = 250\n",
    "    ref_pointer = 0\n",
    "    ref_split_count = 0\n",
    "    chapter_bertscore_F1 = 0\n",
    "    while ref_pointer < len(ref_tokenized):\n",
    "        ref_summary_split = ' '.join(ref_tokenized[ref_pointer:min(len(ref_tokenized), ref_pointer + split_length)])\n",
    "        max_candidate_score = -1\n",
    "        candidate_pointer = 0\n",
    "        while candidate_pointer < len(candidate_tokenized):\n",
    "            candidate_summary_split = ' '.join(candidate_tokenized[candidate_pointer:min(len(candidate_tokenized), candidate_pointer + split_length)])\n",
    "            P, R, F1 = bertscorer.score([candidate_summary_split], [ref_summary_split])\n",
    "            #print(\"Candidate length = {}, reference length = {}\".format(len(word_tokenize(candidate_summary_split)), len(word_tokenize(ref_summary_split))))\n",
    "            max_candidate_score = max(max_candidate_score, F1.mean().item())\n",
    "            candidate_pointer += split_length\n",
    "        chapter_bertscore_F1 += max_candidate_score\n",
    "        ref_pointer += split_length\n",
    "        ref_split_count += 1\n",
    "    chapter_bertscore_F1 /= ref_split_count\n",
    "    average_bertscore_F1 += chapter_bertscore_F1\n",
    "    print(\"Chapter BERTScore F1 = {}\".format(chapter_bertscore_F1))\n",
    "    '''\n",
    "average_bertscore_F1 /= len(test_generated_summaries)\n",
    "print(\"Test average BERTscore F1 = {}\".format(average_bertscore_F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a9feed-dcea-4e27-b01b-db97fb93e76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test summaQA\n",
    "#srcs = test_chapters[:len(test_generated_summaries)]\n",
    "#gens = test_generated_summaries\n",
    "#srcs = [' '.join(' '.join(srcs[i].split(\" \")).split()[:300]) for i in range(len(srcs))]\n",
    "#gens = [' '.join(' '.join(gens[i].split(\" \")).split()[:300]) for i in range(len(gens))]\n",
    "#evaluate_corpus(srcs, gens)\n",
    "\n",
    "average_summaqa_prob = 0\n",
    "average_summaqa_F1 = 0\n",
    "\n",
    "for i in range(len(test_generated_summaries)):\n",
    "    print(\"Test doc {}:\".format(i))\n",
    "    \n",
    "    chapter_content = copy.deepcopy(test_chapters[i])\n",
    "    candidate_summary = copy.deepcopy(test_generated_summaries[i])\n",
    "\n",
    "    # Method 1: Split by sentence\n",
    "    chapter_sentences = test_chapters_sentences[i]\n",
    "    candidate_sentences = sent_tokenize(candidate_summary)\n",
    "    chapter_split_count = 0\n",
    "    chapter_summaqa_prob = 0\n",
    "    chapter_summaqa_F1 = 0\n",
    "    for j in range(len(chapter_sentences)):\n",
    "        chapter_summary_split = chapter_sentences[j]\n",
    "        article = chapter_summary_split\n",
    "        masked_questions, answer_spans = question_generator.get_questions(article)\n",
    "        if len(masked_questions) == 0:\n",
    "            continue\n",
    "        chapter_split_count += 1\n",
    "        print(\"Chapter split #{}:\".format(j))\n",
    "        print(chapter_summary_split)\n",
    "        print(\"Questions:\")\n",
    "        print(masked_questions)\n",
    "        print(\"Answers:\")\n",
    "        print(answer_spans)\n",
    "        max_candidate_F1 = 0\n",
    "        max_candidate_prob = 0\n",
    "        #chapter_split_sum_F1 = 0\n",
    "        #chapter_split_sum_prob = 0\n",
    "        for k in range(len(candidate_sentences)):\n",
    "            candidate_summary_split = candidate_sentences[k]\n",
    "            summaqa_scores = qa_metric.compute(masked_questions, answer_spans, candidate_summary_split)\n",
    "            max_candidate_F1 = max(max_candidate_F1, summaqa_scores['avg_fscore'])\n",
    "            max_candidate_prob = max(max_candidate_prob, summaqa_scores['avg_prob'])\n",
    "            #chapter_split_sum_F1 += summaqa_scores['avg_fscore']\n",
    "            #chapter_split_sum_prob += summaqa_scores['avg_prob']\n",
    "            print(\"Candidate split #{} (Chapter split length = {}, candidate length = {}): SummaQA score = {}\".format(k, len(word_tokenize(chapter_summary_split)), len(word_tokenize(candidate_summary_split)), summaqa_scores))\n",
    "            print(candidate_summary_split)\n",
    "        print(\"Chapter split max prob = {}\".format(max_candidate_prob))\n",
    "        print(\"Chapter split max F1 = {}\".format(max_candidate_F1))\n",
    "        #print(\"Chapter split sum prob = {}\".format(chapter_split_sum_prob))\n",
    "        #print(\"Chapter split sum F1 = {}\".format(chapter_split_sum_F1))\n",
    "        chapter_summaqa_prob += max_candidate_prob\n",
    "        chapter_summaqa_F1 += max_candidate_F1\n",
    "        #chapter_summaqa_prob += chapter_split_sum_prob\n",
    "        #chapter_summaqa_F1 += chapter_split_sum_F1\n",
    "    if chapter_split_count > 0:\n",
    "        chapter_summaqa_prob /= chapter_split_count\n",
    "        chapter_summaqa_F1 /= chapter_split_count\n",
    "    else:\n",
    "        chapter_summaqa_prob = 1\n",
    "        chapter_summaqa_F1 = 1\n",
    "    average_summaqa_prob += chapter_summaqa_prob\n",
    "    average_summaqa_F1 += chapter_summaqa_F1\n",
    "    print(\"Chapter SummaQA prob = {}\".format(chapter_summaqa_prob))\n",
    "    print(\"Chapter SummaQA F1 = {}\".format(chapter_summaqa_F1))\n",
    "    \n",
    "    '''\n",
    "    # Method 2: Split by n words\n",
    "    chapter_tokenized = word_tokenize(chapter_content)\n",
    "    candidate_tokenized = word_tokenize(candidate_summary)\n",
    "    split_length = 250\n",
    "    chapter_pointer = 0\n",
    "    chapter_split_count = 0\n",
    "    chapter_summaqa_prob = 0\n",
    "    chapter_summaqa_F1 = 0\n",
    "    while chapter_pointer < len(chapter_tokenized):\n",
    "        chapter_summary_split = ' '.join(chapter_tokenized[chapter_pointer:min(len(chapter_tokenized), chapter_pointer + split_length)])\n",
    "        article = chapter_summary_split\n",
    "        masked_questions, answer_spans = question_generator.get_questions(article)\n",
    "        print(\"Chapter split #{}:\".format(chapter_split_count))\n",
    "        print(chapter_summary_split)\n",
    "        print(\"Questions:\")\n",
    "        print(masked_questions)\n",
    "        print(\"Answers:\")\n",
    "        print(answer_spans)\n",
    "        #max_candidate_score = -1\n",
    "        chapter_split_sum_F1 = 0\n",
    "        chapter_split_sum_prob = 0\n",
    "        candidate_pointer = 0\n",
    "        while candidate_pointer < len(candidate_tokenized):\n",
    "            candidate_summary_split = ' '.join(candidate_tokenized[candidate_pointer:min(len(candidate_tokenized), candidate_pointer + split_length)])\n",
    "            summaqa_scores = qa_metric.compute(masked_questions, answer_spans, candidate_summary_split)\n",
    "            #max_candidate_score = max(max_candidate_score, summaqa_scores['avg_fscore'])\n",
    "            chapter_split_sum_F1 += summaqa_scores['avg_fscore']\n",
    "            chapter_split_sum_prob += summaqa_scores['avg_prob']\n",
    "            print(\"Candidate split #{} (Chapter split length = {}, candidate length = {}): SummaQA score = {}\".format(math.floor(candidate_pointer / split_length), len(word_tokenize(chapter_summary_split)), len(word_tokenize(candidate_summary_split)), summaqa_scores))\n",
    "            print(candidate_summary_split)\n",
    "            candidate_pointer += split_length\n",
    "        print(\"Chapter split sum prob = {}\".format(chapter_split_sum_prob))\n",
    "        print(\"Chapter split sum F1 = {}\".format(chapter_split_sum_F1))\n",
    "        chapter_summaqa_prob += chapter_split_sum_prob\n",
    "        chapter_summaqa_F1 += chapter_split_sum_F1\n",
    "        #chapter_summaqa_F1 += max_candidate_score\n",
    "        chapter_pointer += split_length\n",
    "        chapter_split_count += 1\n",
    "    chapter_summaqa_prob /= chapter_split_count\n",
    "    chapter_summaqa_F1 /= chapter_split_count\n",
    "    average_summaqa_prob += chapter_summaqa_prob\n",
    "    average_summaqa_F1 += chapter_summaqa_F1\n",
    "    print(\"Chapter SummaQA prob = {}\".format(chapter_summaqa_prob))\n",
    "    print(\"Chapter SummaQA F1 = {}\".format(chapter_summaqa_F1))\n",
    "    '''\n",
    "\n",
    "average_summaqa_prob /= len(test_generated_summaries)\n",
    "average_summaqa_F1 /= len(test_generated_summaries)\n",
    "print(\"Test average SummaQA prob = {}\".format(average_summaqa_prob))\n",
    "print(\"Test average SummaQA F1 = {}\".format(average_summaqa_F1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a297f2a3-c316-4551-85d2-db5b14310f16",
   "metadata": {},
   "source": [
    "## End of [6']: Run test & evaluate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
